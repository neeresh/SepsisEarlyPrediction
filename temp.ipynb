{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-13T16:18:08.144263Z",
     "start_time": "2024-09-13T16:18:01.556996Z"
    }
   },
   "source": [
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import compute_class_weight\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from models.custom_models.gtn import GatedTransformerNetwork\n",
    "from models.gtn.transformer import Transformer\n",
    "from utils.config import gtn_param\n",
    "from utils.loader import make_loader\n",
    "from utils.path_utils import project_root\n",
    "from utils.plot_metrics import plot_losses_and_accuracies\n",
    "\n",
    "device = 'cuda'\n",
    "config = gtn_param\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:19:29.079119Z",
     "start_time": "2024-09-13T16:19:29.068293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def train_model(model, train_loader: DataLoader, test_loader: DataLoader, epochs: int, class_0_weight=None,\n",
    "                class_1_weight=None, val_loader: Optional[DataLoader] = None):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # GTN\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=config['lr'])  # GTN\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=epochs)  # Not in GTN Implementation\n",
    "\n",
    "    train_losses, val_losses, test_losses = [], [], []\n",
    "    train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train, total_train = 0, 0\n",
    "\n",
    "        train_loader_tqdm = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "        for inputs, labels in train_loader_tqdm:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, _, _, _, _, _, _ = model(inputs.to(device).to(torch.float32), 'train')\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted.detach().cpu() == labels).sum().item()\n",
    "\n",
    "            # Update tqdm description for training progress\n",
    "            train_loader_tqdm.set_postfix({\n",
    "                \"Train Loss\": running_train_loss / total_train,\n",
    "                \"Train Acc\": correct_train / total_train\n",
    "            })\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        epoch_train_accuracy = correct_train / total_train\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            correct_val, total_val = 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs, _, _, _, _, _, _ = model(inputs.to(torch.float32), 'test')\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_val_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted.detach().cpu() == labels).sum().item()\n",
    "\n",
    "            epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            epoch_val_accuracy = correct_val / total_val\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accuracies.append(epoch_val_accuracy)\n",
    "        else:\n",
    "            epoch_val_loss = \"N/A\"\n",
    "            epoch_val_accuracy = \"N/A\"\n",
    "\n",
    "        # epoch_val_loss = \"N/A\"  # Remove when uncommenting above code\n",
    "        # epoch_val_accuracy = \"N/A\"  # Remove when uncommenting above code\n",
    "\n",
    "        # Testing phase\n",
    "        running_test_loss = 0.0\n",
    "        correct_test, total_test = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs, _, _, _, _, _, _ = model(inputs.to(device).to(torch.float32), 'test')\n",
    "                loss = criterion(outputs, labels.to(device))\n",
    "                running_test_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted.detach().cpu() == labels).sum().item()\n",
    "\n",
    "        epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "        epoch_test_accuracy = correct_test / total_test\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "        # scheduler.step()  # Not in GTN Implementation\n",
    "\n",
    "        message = f\"Epoch {epoch + 1}/{epochs} - \" \\\n",
    "                  f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, \" \\\n",
    "                  f\"Val Loss: {epoch_val_loss}, Val Acc: {epoch_val_accuracy}, \" \\\n",
    "                  f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_accuracy:.4f}\"\n",
    "        tqdm.tqdm.write(message)\n",
    "        logging.info(message)\n",
    "\n",
    "    # Saving the model\n",
    "    save_model(model, model_name=f\"./saved_models/gtn/yingzhe_exp_settings_{config['num_epochs']}.pkl\")\n",
    "\n",
    "    return {\"train_loss\": train_losses, \"val_loss\": val_losses if val_loader else None, \"test_loss\": test_losses,\n",
    "            \"train_accuracy\": train_accuracies, \"val_accuracy\": val_accuracies if val_loader else None,\n",
    "            \"test_accuracy\": test_accuracies}\n",
    "\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    logging.info(f\"Saving the model with model_name: {model_name}\")\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "\n",
    "    logging.info(f\"Saving successfull!!!\")\n",
    "\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    device = 'cuda'\n",
    "    print(f\"Loading {model_name} GTN model...\")\n",
    "    logging.info(f\"Loading GTN model...\")\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    print(f\"Model is set to eval() mode...\")\n",
    "    logging.info(f\"Model is set to eval() mode...\")\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model is on the deivce: {device}\")\n",
    "    logging.info(f\"Model is on the deivce: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    return model\n"
   ],
   "id": "cfdc1c58a27b50a3",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "8b48ba556956108d",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:21:13.839504Z",
     "start_time": "2024-09-13T16:21:13.809420Z"
    }
   },
   "cell_type": "code",
   "source": "pd.read_csv('./data/yingzhe/balanced_data/train/sepsis/p014430.psv', sep='|')",
   "id": "3943dbb4913e060a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.613036Z",
     "start_time": "2024-09-13T16:13:35.611459Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "800257ccc140c7b3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.615277Z",
     "start_time": "2024-09-13T16:13:35.613875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(f\"Using {torch.cuda.device_count()} GPUs...\")\n",
    "\n",
    "# Getting Data and Loaders\n",
    "data_file = \"final_dataset.pickle\"\n",
    "training_examples, lengths_list, is_sepsis, writer, destination_path = initialize_experiment(data_file)\n",
    "\n",
    "sepsis = pd.Series(is_sepsis)\n",
    "positive_sepsis_idxs = sepsis[sepsis == 1].index\n",
    "# negative_sepsis_idxs = sepsis[sepsis == 0].sample(frac=0.20).index\n",
    "negative_sepsis_idxs = sepsis[sepsis == 0].sample(frac=0.50).index\n",
    "all_samples = list(positive_sepsis_idxs) + list(negative_sepsis_idxs)\n",
    "np.random.shuffle(all_samples)\n",
    "\n",
    "print(f\"Number of positive samples: {len(positive_sepsis_idxs)}\")\n",
    "print(f\"Number of negative samples: {len(negative_sepsis_idxs)}\")\n",
    "\n",
    "# Reducing the samples to have balanced dataset\n",
    "batch_size = config['batch_size'] * torch.cuda.device_count()\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "logging.info(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Splitting dataset into train and test\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "\n",
    "train_indicies, temp_indicies = train_test_split(all_samples, test_size=0.4, random_state=42)  # 60 40\n",
    "val_indicies, test_indicies = train_test_split(temp_indicies, test_size=0.5, random_state=42)  # 20 20\n",
    "\n",
    "# train_indicies, test_indicies = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "# train_loader, test_loader, train_indicies, test_indicies = make_loader(training_examples, lengths_list, is_sepsis,\n",
    "#                                                                        batch_size=batch_size, mode='padding',\n",
    "#                                                                        num_workers=4, train_indicies=train_indicies,\n",
    "#                                                                        test_indicies=test_indicies, include_val=True)\n",
    "\n",
    "train_loader, val_loader, test_loader, train_indicies, val_indices, test_indicies = make_loader(\n",
    "    training_examples, lengths_list, is_sepsis, batch_size=batch_size, mode='padding', num_workers=4,\n",
    "    train_indicies=train_indicies, test_indicies=test_indicies, val_indicies=val_indicies,\n",
    "    select_important_features=False, include_val=True)\n",
    "\n",
    "# Model's input shape\n",
    "(d_input, d_channel), d_output = train_loader.dataset.data[0].shape, 2  # (time_steps, features, num_classes)\n",
    "print(f\"d_input: {d_input}, d_channel: {d_channel}, d_output: {d_output}\")\n",
    "num_epochs = config['num_epochs']\n",
    "\n",
    "print(d_input, d_channel, d_output)\n",
    "\n",
    "logging.info(config)\n",
    "logging.info(f\"d_input: {d_input}, d_channel: {d_channel}, d_output: {d_output}\")\n",
    "logging.info(f\"Number of epochs: {num_epochs}\")\n",
    "\n",
    "model = Transformer(d_model=config['d_model'], d_input=d_input, d_channel=d_channel,\n",
    "                    d_output=d_output, d_hidden=config['d_hidden'], q=config['q'],\n",
    "                    v=config['v'], h=config['h'], N=config['N'], dropout=config['dropout'],\n",
    "                    pe=config['pe'], mask=config['mask'], device=device).to(device)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# class_0_weight = len(all_samples) / (len(negative_sepsis_idxs) * d_output)\n",
    "# class_1_weight = len(all_samples) / (len(positive_sepsis_idxs) * d_output)\n",
    "# metrics = train_model(model, train_loader, test_loader, class_0_weight=class_0_weight,\n",
    "#                       class_1_weight=class_1_weight, epochs=num_epochs)\n",
    "\n",
    "classes = np.unique(np.array(train_loader.dataset.labels).ravel())\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes,\n",
    "                                     y=np.array(train_loader.dataset.labels))\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to('cuda')\n",
    "\n",
    "metrics = train_model(model, train_loader, val_loader, epochs=num_epochs, class_0_weight=class_weights_tensor[0],\n",
    "                      class_1_weight=class_weights_tensor[1])\n",
    "\n",
    "# Save test files in /localhost/.../test_data\n",
    "import glob\n",
    "\n",
    "test_data_path = os.path.join(project_root(), 'data', 'test_data', 'gtn')\n",
    "os.makedirs(test_data_path, exist_ok=True)\n",
    "\n",
    "# Removing all files before uploading test data\n",
    "for file_path in glob.glob(f\"{test_data_path}/*\"):\n",
    "    os.remove(file_path)\n",
    "\n",
    "# Gathering all files\n",
    "files_names = []\n",
    "input_directory = [\n",
    "    os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training', 'training_setA'),\n",
    "    os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training', 'training_setB')\n",
    "]\n",
    "\n",
    "# Loading all file names and sort\n",
    "for dir in input_directory:\n",
    "    for f in os.listdir(dir):\n",
    "        file_path = os.path.join(dir, f)\n",
    "        if os.path.isfile(file_path) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "            files_names.append(file_path)\n",
    "\n",
    "files_names.sort()\n",
    "\n",
    "# Saving test data\n",
    "for idx in tqdm.tqdm(test_indicies, desc=\"Saving test data in 'test_data' directory\", total=len(test_indicies)):\n",
    "    patient_file = files_names[idx]\n",
    "    patient_name = os.path.basename(patient_file)\n",
    "    patient_data = pd.read_csv(patient_file, delimiter='|')\n",
    "\n",
    "    output_file_path = os.path.join(test_data_path, patient_name)\n",
    "    patient_data.to_csv(output_file_path, index=False, sep='|')\n",
    "\n",
    "# Metrics\n",
    "train_losses, val_losses, test_losses, = metrics['train_loss'], metrics['val_loss'], metrics['test_loss']\n",
    "train_accuracies, val_accuracies, test_accuracies = metrics['train_accuracy'], metrics['val_accuracy'], metrics[\n",
    "    'test_accuracy']\n",
    "\n",
    "if 'physionet2019' in destination_path:  # When using Unity\n",
    "\n",
    "    # Saving Locally\n",
    "    plot_losses_and_accuracies(train_losses, test_losses, train_accuracies, test_accuracies,\n",
    "                               save_path='../data/logs')  # Local\n",
    "\n",
    "plot_losses_and_accuracies(train_losses, test_losses, train_accuracies, test_accuracies,\n",
    "                           save_path=destination_path)\n"
   ],
   "id": "c8d92fb5af091b0d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.617304Z",
     "start_time": "2024-09-13T16:13:35.615932Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b1f52ef33cf825d5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.631152Z",
     "start_time": "2024-09-13T16:13:35.629502Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ed486210c020ca5d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.633314Z",
     "start_time": "2024-09-13T16:13:35.631961Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2795c18d79bab44e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.635455Z",
     "start_time": "2024-09-13T16:13:35.634128Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a9fffdd44e67180a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.637381Z",
     "start_time": "2024-09-13T16:13:35.636090Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f7069cc6b5bc57e0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:13:35.639596Z",
     "start_time": "2024-09-13T16:13:35.638286Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8d9a46a28a80c698",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "f608e6112672dbc5",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
