{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-15T07:21:16.528424Z",
     "start_time": "2024-09-15T07:21:16.526913Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T03:05:41.742040Z",
     "start_time": "2024-09-17T03:05:37.831713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from utils.path_utils import project_root\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n"
   ],
   "id": "e26363a27c9bea16",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:21:20.002992Z",
     "start_time": "2024-09-15T07:21:20.001272Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "35310e36b03c6b19",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:35:31.802255Z",
     "start_time": "2024-09-15T06:34:08.242344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.pretrain_utils.data import get_pretrain_finetune_test_datasets\n",
    "\n",
    "pt_train, finetune, test = get_pretrain_finetune_test_datasets()"
   ],
   "id": "9a29c339fca5d558",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:37:22.441190Z",
     "start_time": "2024-09-15T06:37:22.439357Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9b37998e87bf5872",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:49:52.473939Z",
     "start_time": "2024-09-15T06:49:52.472098Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5233f8fc26cbe6fb",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:40:11.194256Z",
     "start_time": "2024-09-15T06:40:11.192537Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e2a43b014aa8152d",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T18:22:15.389626Z",
     "start_time": "2024-09-23T18:22:15.136683Z"
    }
   },
   "cell_type": "code",
   "source": "!sbatch eval_simmtm.job",
   "id": "2f58dbfac2ed3df3",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T02:06:47.919688Z",
     "start_time": "2024-09-15T02:06:47.917946Z"
    }
   },
   "cell_type": "code",
   "source": "-",
   "id": "26f22e2c11ae962d",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "711daf7b9fb53cc3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Args",
   "id": "f4d653dde45d4dc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "parser.add_argument('--run_description', default='run1', type=str, help='Experiment Description')\n",
    "parser.add_argument('--seed', default=2023, type=int, help='seed value')\n",
    "\n",
    "parser.add_argument('--training_mode', default='pre_train', type=str, help='pre_train, fine_tune')\n",
    "parser.add_argument('--pretrain_dataset', default='SleepEEG', type=str,\n",
    "                    help='Dataset of choice: SleepEEG, FD_A, HAR, ECG')\n",
    "parser.add_argument('--target_dataset', default='Epilepsy', type=str,\n",
    "                    help='Dataset of choice: Epilepsy, FD_B, Gesture, EMG')\n",
    "\n",
    "parser.add_argument('--logs_save_dir', default='experiments_logs', type=str, help='saving directory')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='cpu or cuda')\n",
    "parser.add_argument('--home_path', default=home_dir, type=str, help='Project home directory')\n",
    "parser.add_argument('--subset', action='store_true', default=False, help='use the subset of datasets')\n",
    "parser.add_argument('--log_epoch', default=5, type=int, help='print loss and metrix')\n",
    "parser.add_argument('--draw_similar_matrix', default=10, type=int, help='draw similarity matrix')\n",
    "parser.add_argument('--pretrain_lr', default=0.0001, type=float, help='pretrain learning rate')\n",
    "parser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
    "parser.add_argument('--use_pretrain_epoch_dir', default=None, type=str,\n",
    "                    help='choose the pretrain checkpoint to finetune')\n",
    "parser.add_argument('--pretrain_epoch', default=10, type=int, help='pretrain epochs')\n",
    "parser.add_argument('--finetune_epoch', default=300, type=int, help='finetune epochs')\n",
    "\n",
    "parser.add_argument('--masking_ratio', default=0.5, type=float, help='masking ratio')\n",
    "parser.add_argument('--positive_nums', default=3, type=int, help='positive series numbers')\n",
    "parser.add_argument('--lm', default=3, type=int, help='average masked lenght')\n",
    "\n",
    "parser.add_argument('--finetune_result_file_name', default=\"finetune_result.json\", type=str,\n",
    "                    help='finetune result json name')\n",
    "parser.add_argument('--temperature', type=float, default=0.2, help='temperature')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ],
   "id": "8f0f9109afd5d6f0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f43119c437903e8c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert csv to pt",
   "id": "2bae8b6156fcaa81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def csv_to_pt(patient_files, lengths, is_sepsis, desc):\n",
    "    \n",
    "    all_patients = {'samples': [], 'labels': []}\n",
    "    \n",
    "    max_time_step = 336\n",
    "    # print(len(patient_files), len(lengths), len(is_sepsis))\n",
    "    for idx, (file, length, sepsis) in tqdm.tqdm(enumerate(zip(patient_files, lengths, is_sepsis)), \n",
    "                                                      desc=f\"{desc}\", \n",
    "                                                      total=len(patient_files)):\n",
    "        \n",
    "        pad_width = ((0, max_time_step - len(file)), (0, 0))\n",
    "        file = np.pad(file, pad_width=pad_width, mode='constant').astype(np.float32)\n",
    "        \n",
    "        if len(file) == max_time_step:\n",
    "            all_patients['samples'].append(torch.from_numpy(file).unsqueeze(0))\n",
    "            all_patients['labels'].append(torch.tensor(sepsis, dtype=torch.float32).unsqueeze(0))\n",
    "        else:\n",
    "            raise ValueError(f\"Length {length} does not match length of patient {idx} with length {len(file)}\")\n",
    "    \n",
    "    # print('samples: ', type(all_patients['samples']), 'labels: ', type(all_patients['labels']))\n",
    "    \n",
    "    all_patients['samples'] = torch.cat(all_patients['samples'], dim=0)\n",
    "    all_patients['labels'] = torch.cat(all_patients['labels'], dim=0)\n",
    "    \n",
    "    return {'samples': all_patients['samples'], 'labels': all_patients['labels']}, lengths, is_sepsis\n",
    "\n",
    "# all_patients, lengths, is_sepsis = csv_to_pt()\n"
   ],
   "id": "123cbaf79652006c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "43753974148ee5d9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_val_test_indices(sepsis_file, dset, save_distributions=True):\n",
    "    \n",
    "    sepsis = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', f'{sepsis_file}'), header=None)\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(sepsis, test_size=0.2, random_state=2024)\n",
    "    \n",
    "    train_indices = train_indices.index.values\n",
    "    val_indices = val_indices.index.values\n",
    "    \n",
    "    \n",
    "    if save_distributions:\n",
    "        train_dist = sepsis.iloc[train_indices].value_counts()\n",
    "        val_dist = sepsis.iloc[val_indices].value_counts()\n",
    "        \n",
    "        train_dist_percentage = np.round(train_dist / len(sepsis.iloc[train_indices]), 2)\n",
    "        val_dist_percentage = np.round(val_dist / len(sepsis.iloc[val_indices]), 2)\n",
    "        \n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                'Train Images': train_dist, 'Validation Images': val_dist,\n",
    "                'Train Distribution Percentage': train_dist_percentage, 'Validation Distribution Percentage': val_dist_percentage,\n",
    "            }\n",
    "        ).to_csv(os.path.join(project_root(), 'results', f'distributions{dset}.csv'), index=False)\n",
    "        \n",
    "        # pd.read_csv(os.path.join(project_root(), 'results', 'distributions.csv'))\n",
    "        \n",
    "    return train_indices, val_indices\n"
   ],
   "id": "717b9505d28c8ee8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "10aee72c9dbf0a4a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasetup",
   "id": "fbb5c03280f398db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_pretrain_finetune_datasets():\n",
    "    \n",
    "    # Pre-training Indices\n",
    "    pt_train_indices, pt_val_indices = get_train_val_test_indices(\n",
    "        sepsis_file='is_sepsis_pretrain_A.txt', save_distributions=True,\n",
    "        dset='Aa')\n",
    "    \n",
    "    # Gathering files, lengths, and sepsis label\n",
    "    pt_files = pd.read_pickle(os.path.join(project_root(), 'data', 'tl_datasets', 'final_dataset_pretrain_A.pickle'))\n",
    "    pt_lengths = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'lengths_pretrain_A.txt'), \n",
    "                             header=None)\n",
    "    pt_sepsis = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'is_sepsis_pretrain_A.txt'),\n",
    "                            header=None)\n",
    "    \n",
    "    # Checking whether the files are in same order or not\n",
    "    pretrain_files = []\n",
    "    for pdata, length in tqdm.tqdm(zip(pt_files, pt_lengths.values), desc=\"Checking Pre-training & Validation Files\", \n",
    "                                   total=len(pt_files)):\n",
    "        plength = len(pdata) \n",
    "        assert plength == length[0], f\"{plength} doesn't match {length}\"\n",
    "        pretrain_files.append(pdata.drop(['PatientID', 'SepsisLabel'], axis=1))\n",
    "    \n",
    "    # Getting train and val\n",
    "    pt_train = [pretrain_files[i] for i in pt_train_indices]\n",
    "    pt_val = [pretrain_files[i] for i in pt_val_indices]\n",
    "    \n",
    "    pt_train_lengths = pt_lengths.iloc[pt_train_indices].values\n",
    "    pt_val_lengths = pt_lengths.iloc[pt_val_indices].values\n",
    "    \n",
    "    pt_train_sepsis = pt_sepsis.iloc[pt_train_indices].values\n",
    "    pt_val_sepsis = pt_sepsis.iloc[pt_val_indices].values\n",
    "    \n",
    "    pt_train, pt_train_lengths, pt_train_sepsis = csv_to_pt(pt_train, pt_train_lengths, pt_train_sepsis, desc='PT Train Set')\n",
    "    pt_val, pt_val_lengths, pt_val_sepsis = csv_to_pt(pt_val, pt_val_lengths, pt_val_sepsis, desc='PT Validation Set')\n",
    "    \n",
    "    # Fine-tuning\n",
    "    test_indices, finetune_indices = get_train_val_test_indices(\n",
    "        sepsis_file='is_sepsis_finetune_B.txt', save_distributions=True,\n",
    "        dset='Bb')\n",
    "    \n",
    "    # Gathering files, lengths, and sepsis label\n",
    "    test_setB = pd.read_pickle(os.path.join(project_root(), 'data', 'tl_datasets', 'final_dataset_finetune_B.pickle'))\n",
    "    test_setB_lengths = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'lengths_finetune_B.txt'), \n",
    "                             header=None)\n",
    "    test_setB_sepsis = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'is_sepsis_finetune_B.txt'),\n",
    "                            header=None)\n",
    "    \n",
    "    # Checking whether the files are in same order or not\n",
    "    test_files = []\n",
    "    for pdata, length in tqdm.tqdm(zip(test_setB, test_setB_lengths.values), desc=\"Checking Fine-tuning & Test Files\",\n",
    "                                   total=len(test_setB)):\n",
    "        plength = len(pdata) \n",
    "        assert plength == length[0], f\"{plength} doesn't match {length}\"\n",
    "        test_files.append(pdata.drop(['PatientID', 'SepsisLabel'], axis=1))\n",
    "    \n",
    "    # Getting finetune and test sets\n",
    "    finetune = [test_files[i] for i in finetune_indices]\n",
    "    test = [test_files[i] for i in test_indices]\n",
    "    \n",
    "    finetune_lengths = test_setB_lengths.iloc[finetune_indices].values\n",
    "    test_lengths = test_setB_lengths.iloc[test_indices].values\n",
    "    \n",
    "    finetune_sepsis = test_setB_sepsis.iloc[finetune_indices].values\n",
    "    test_sepsis = test_setB_sepsis.iloc[test_indices].values\n",
    "    \n",
    "    finetune, finetune_lengths, finetune_sepsis = csv_to_pt(finetune, finetune_lengths, finetune_sepsis, desc=\"Fine-tuning Set\")\n",
    "    test, test_lengths, test_sepsis = csv_to_pt(test, test_lengths, test_sepsis, desc=\"Test Set\")\n",
    "    \n",
    "    print(\"Pre-training samples: \", pt_train['samples'].shape, \"Validation samples: \", pt_val['samples'].shape)\n",
    "    print(\"Fine-tuning samples: \", finetune['samples'].shape, \"Test samples: \", test['samples'].shape)\n",
    "    \n",
    "    return pt_train, pt_val, finetune, test\n",
    "    \n",
    "pt_train, pt_val, finetune, test = get_pretrain_finetune_datasets()\n"
   ],
   "id": "687ad37dc749028d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b8503e477808e77e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "    Returns:\n",
    "        (L, ) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (\n",
    "            1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask\n",
    "\n",
    "\n",
    "def noise_mask(X, masking_ratio=0.25, lm=3, distribution='geometric', exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n",
    "        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n",
    "            feat_dim that will be masked on average\n",
    "        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n",
    "        distribution: whether each mask sequence element is sampled independently at random, or whether\n",
    "            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n",
    "            masked squences of a desired mean length `lm`\n",
    "        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n",
    "    Returns:\n",
    "        boolean numpy array with the same shape as X, with 0s at places where a feature should be masked\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        mask = geom_noise_mask_single(X.shape[0] * X.shape[1] * X.shape[2], lm, masking_ratio)\n",
    "        mask = mask.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "        \n",
    "    elif distribution == 'masked_tail':\n",
    "        mask = np.ones(X.shape, dtype=bool)\n",
    "        for m in range(X.shape[0]):  # feature dimension\n",
    "\n",
    "            keep_mask = np.zeros_like(mask[m, :], dtype=bool)\n",
    "            n = math.ceil(keep_mask.shape[1] * (1 - masking_ratio))\n",
    "            keep_mask[:, :n] = True\n",
    "            mask[m, :] = keep_mask  # time dimension\n",
    "            \n",
    "    elif distribution == 'masked_head':\n",
    "        mask = np.ones(X.shape, dtype=bool)\n",
    "        for m in range(X.shape[0]):  # feature dimension\n",
    "\n",
    "            keep_mask = np.zeros_like(mask[m, :], dtype=bool)\n",
    "            n = math.ceil(keep_mask.shape[1] * masking_ratio)\n",
    "            keep_mask[:, n:] = True\n",
    "            mask[m, :] = keep_mask  # time dimension\n",
    "    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n",
    "        mask = np.random.choice(np.array([True, False]), size=X.shape, replace=True,\n",
    "                                p=(1 - masking_ratio, masking_ratio))\n",
    "\n",
    "    return torch.tensor(mask)\n",
    "\n",
    "def data_transform_masked4cl(sample, masking_ratio, lm, positive_nums=None, distribution='geometric'):\n",
    "    \"\"\"Masked time series in time dimension\"\"\"\n",
    "\n",
    "    if positive_nums is None:\n",
    "        positive_nums = math.ceil(1.5 / (1 - masking_ratio))\n",
    "        \n",
    "    sample = sample.permute(0, 2, 1)  # (batch_size, channels, time_steps)\n",
    "    \n",
    "    # Creating the batch in #positive_nums sets\n",
    "    sample_repeat = sample.repeat(positive_nums, 1, 1)  # (batch_size*positive_num, channels, time steps)\n",
    "\n",
    "    mask = noise_mask(sample_repeat, masking_ratio, lm, distribution=distribution)\n",
    "    x_masked = mask * sample_repeat\n",
    "\n",
    "    return x_masked.permute(0, 2, 1), mask.permute(0, 2, 1)\n",
    "\n",
    "# data_masked_m, mask = data_transform_masked4cl(all_patients['samples'][:32], 0.5, 3, positive_nums=1, distribution='geometric')\n"
   ],
   "id": "df2ee9f255cd3528",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e14802c72ae0ed5c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Load_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, TSlength_aligned, training_mode):\n",
    "        \n",
    "        super(Load_Dataset, self).__init__()\n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        X_train = dataset[\"samples\"]\n",
    "        y_train = dataset[\"labels\"]\n",
    "        \n",
    "        # shuffle\n",
    "        data = list(zip(X_train, y_train))\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        X_train, y_train = zip(*data)\n",
    "        X_train, y_train = torch.stack(list(X_train), dim=0), torch.stack(list(y_train), dim=0)\n",
    "\n",
    "        if len(X_train.shape) < 3:\n",
    "            X_train = X_train.unsqueeze(2)\n",
    "\n",
    "        # if X_train.shape.index(min(X_train.shape)) != 1:  # make sure the Channels in second dim\n",
    "        #     X_train = X_train.permute(0, 2, 1)\n",
    "\n",
    "        \"\"\"Align the TS length between source and target datasets\"\"\"\n",
    "        # X_train = X_train[:, :1, :int(config.TSlength_aligned)] # take the first 178 samples\n",
    "        X_train = X_train[:, :, :int(TSlength_aligned)]\n",
    "        \n",
    "        if isinstance(X_train, np.ndarray):\n",
    "            self.x_data = torch.from_numpy(X_train)\n",
    "            self.y_data = torch.from_numpy(y_train).long()\n",
    "        else:\n",
    "            self.x_data = X_train\n",
    "            self.y_data = y_train\n",
    "\n",
    "        self.len = X_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ],
   "id": "813e977f40d98345",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1a86d99d645f058",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.simmtm.gtn.encoder import Encoder\n",
    "from simmtm.loss import ContrastiveWeight, AggregationRebuild, AutomaticWeightedLoss\n",
    "\n",
    "\n",
    "class TFC(nn.Module):\n",
    "    def __init__(self, configs, args):\n",
    "        \n",
    "        super(TFC, self).__init__()\n",
    "        self.training_mode = 'pre_train'\n",
    "        \n",
    "        # Projecting input into deep representations\n",
    "        self.encoder_list_1 = ModuleList([Encoder(d_model=configs.d_model, d_hidden=configs.d_hidden, q=configs.q,\n",
    "                                                  v=configs.v, h=configs.h, mask=configs.mask, dropout=configs.dropout,\n",
    "                                                  device=configs.device) for _ in range(configs.N)])\n",
    "\n",
    "        self.encoder_list_2 = ModuleList([Encoder(d_model=configs.d_model, d_hidden=configs.d_hidden, q=configs.q,\n",
    "                                                  v=configs.v, h=configs.h, dropout=configs.dropout,\n",
    "                                                  device=configs.device) for _ in range(configs.N)])\n",
    "\n",
    "        self.embedding_channel = torch.nn.Linear(configs.d_channel, configs.d_model)\n",
    "        self.embedding_input = torch.nn.Linear(configs.d_input, configs.d_model)\n",
    "\n",
    "        self.gate = torch.nn.Linear(configs.d_model * configs.d_input + configs.d_model * configs.d_channel,\n",
    "                                    configs.d_output)\n",
    "\n",
    "        self.pe = configs.pe\n",
    "        self._d_input = configs.d_input\n",
    "        self._d_model = configs.d_model\n",
    "\n",
    "        # MLP Layer - To generate Projector(.); to Obtain series-wise representations\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(192512, 256),  # 240128 = encoder1 out features + encoder2 out features\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "        if self.training_mode == 'pre_train':\n",
    "            self.awl = AutomaticWeightedLoss(2)\n",
    "            self.contrastive = ContrastiveWeight(args)\n",
    "            self.aggregation = AggregationRebuild(args)\n",
    "            # self.head = nn.Linear(240128, 336)  # Reconstruction, we have 336 time steps\n",
    "            self.head = nn.Linear(192512, configs.d_input * configs.d_channel)  # Replaced to handle multi-variate\n",
    "            self.mse = torch.nn.MSELoss()\n",
    "            \n",
    "    def forward(self, stage, x_in_t, pre_train=False):\n",
    "\n",
    "        # x_in_t: (128, 336, 133)\n",
    "        encoding_1 = self.embedding_channel(x_in_t)  # (128, 336, 512)\n",
    "        input_to_gather = encoding_1 \n",
    "\n",
    "        if self.pe:\n",
    "            pe = torch.ones_like(encoding_1[0])\n",
    "            position = torch.arange(0, self._d_input).unsqueeze(-1)\n",
    "            temp = torch.Tensor(range(0, self._d_model, 2))\n",
    "            temp = temp * -(math.log(10000) / self._d_model)\n",
    "            temp = torch.exp(temp).unsqueeze(0)\n",
    "            temp = torch.matmul(position.float(), temp)  # shape:[input, d_model/2]\n",
    "            pe[:, 0::2] = torch.sin(temp)\n",
    "            pe[:, 1::2] = torch.cos(temp)\n",
    "\n",
    "            encoding_1 = encoding_1 + pe  # (128, 336, 512)\n",
    "\n",
    "        for encoder in self.encoder_list_1:\n",
    "            # encoding_1: (128, 336, 512)\n",
    "            encoding_1, score_input = encoder(encoding_1, stage)\n",
    "            \n",
    "        encoding_2 = self.embedding_input(x_in_t.transpose(-1, -2))  # encoding_2: (128, 133, 512)\n",
    "        channel_to_gather = encoding_2  \n",
    "\n",
    "        for encoder in self.encoder_list_2:\n",
    "            # encoding_2: (128, 133, 512)\n",
    "            encoding_2, score_channel = encoder(encoding_2, stage)\n",
    "\n",
    "        encoding_1 = encoding_1.reshape(encoding_1.shape[0], -1)  # (128, 172032)\n",
    "        encoding_2 = encoding_2.reshape(encoding_2.shape[0], -1)  # (128, 68096)\n",
    "        \n",
    "        encoding_concat = self.gate(torch.cat([encoding_1, encoding_2], dim=-1))  # (128, 2)\n",
    "        \n",
    "        # gate: torch.Size([128, 2])\n",
    "        gate = F.softmax(encoding_concat, dim=-1)  \n",
    "        encoding = torch.cat([encoding_1 * gate[:, 0:1], encoding_2 * gate[:, 1:2]], dim=-1)  # (128, 240128)\n",
    "        # print(encoding.shape)\n",
    "        \n",
    "        # Projections\n",
    "        projections = self.dense(encoding)  # (128, 128)\n",
    "\n",
    "        if pre_train:\n",
    "            # loss_cl: torch.Size([])\n",
    "            # similarity_matrix: torch.Size([128, 128])\n",
    "            # logits: torch.Size([128, 127])\n",
    "            # positives_mask: torch.Size([128, 128])\n",
    "            loss_cl, similarity_matrix, logits, positives_mask = self.contrastive(projections)           \n",
    "            \n",
    "            # rebuild_weight_matrix: torch.Size([128, 128])\n",
    "            # agg_x: torch.Size([128, 240128])\n",
    "            rebuild_weight_matrix, agg_x = self.aggregation(similarity_matrix, encoding)\n",
    "            \n",
    "            # pred_x: torch.Size([128, 336])\n",
    "            pred_x = self.head(agg_x.reshape(agg_x.size(0), -1))\n",
    "            \n",
    "            # x_in_t.shape: torch.Size([128, 336, 133])\n",
    "            # x_in_t.reshape(x_in_t.size(0), -1): torch.Size([128, 44688])\n",
    "            loss_rb = self.mse(pred_x, x_in_t.reshape(x_in_t.size(0), -1).detach())\n",
    "            loss = self.awl(loss_cl, loss_rb)\n",
    "\n",
    "            return loss, loss_cl, loss_rb\n",
    "\n",
    "        return encoding, encoding_concat\n",
    "    "
   ],
   "id": "5df8708066259139",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f16d7cef8d8db61f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def model_pretrain(model, model_optimizer, model_scheduler, train_loader, configs, args, device):\n",
    "    total_loss = []\n",
    "    total_cl_loss = []\n",
    "    total_rb_loss = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in tqdm.tqdm(enumerate(train_loader), desc=\"Pre-training model\", total=len(train_loader)):  # data shape: (batch_size, seqs, channels)\n",
    "\n",
    "        model_optimizer.zero_grad()\n",
    "        # When masking, data is reshaped to (batch_size, channel, seqs) - Inside the data_transform_masked4cl()\n",
    "        data_masked_m, mask = data_transform_masked4cl(data, args.masking_ratio, args.lm, args.positive_nums)\n",
    "        data_masked_om = torch.cat([data, data_masked_m], 0)  # (batch_size, seqs, channels)\n",
    "\n",
    "        data, labels = data.float().to('cpu'), labels.float().to('cpu')\n",
    "        data_masked_om = data_masked_om.float().to(device)\n",
    "\n",
    "        # Produce embeddings of original and masked samples  (data_masked_om = data samples + masked samples)\n",
    "        # loss, loss_cl, loss_rb = model(data_masked_om, pretrain=True)\n",
    "        # return loss, loss_cl, loss_rb\n",
    "        \n",
    "        loss, loss_cl, loss_rb = model(stage='train', x_in_t=data_masked_om, pre_train=True)\n",
    "        \n",
    "        # return loss, loss_cl, loss_rb\n",
    "\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        total_cl_loss.append(loss_cl.item())\n",
    "        total_rb_loss.append(loss_rb.item())\n",
    "\n",
    "    total_loss = torch.tensor(total_loss).mean()\n",
    "    total_cl_loss = torch.tensor(total_cl_loss).mean()\n",
    "    total_rb_loss = torch.tensor(total_rb_loss).mean()\n",
    "\n",
    "    model_scheduler.step()\n",
    "\n",
    "    return total_loss, total_cl_loss, total_rb_loss\n"
   ],
   "id": "53dd90f7b8f1d93c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70fb02e9daf22921",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pt_dataset = Load_Dataset(pt_train, TSlength_aligned=336, training_mode='pretrain')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=pt_dataset, batch_size=32, shuffle=True, \n",
    "                                           drop_last=True, num_workers=4)  # (32, 336, 40)\n",
    "\n",
    "val_dataset = Load_Dataset(pt_val, TSlength_aligned=336, training_mode='pretrain')\n",
    "val_loader = torch.utils.data.DataLoader(dataset=pt_val, batch_size=32, shuffle=True, \n",
    "                                           drop_last=True, num_workers=4)\n",
    "\n",
    "finetune_dataset = Load_Dataset(finetune, TSlength_aligned=336, training_mode='finetune')\n",
    "finetune_loader = torch.utils.data.DataLoader(finetune_dataset, batch_size=32, shuffle=True, \n",
    "                                              drop_last=True, num_workers=4)\n",
    "\n"
   ],
   "id": "73cf01b5a50301d5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4791a78dc05d8fbe",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "66c8d4e9473a73ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_model_size(model):\n",
    "    \n",
    "    def convert_to_gigabytes(input_megabyte):\n",
    "        gigabyte = 1.0/1024\n",
    "        convert_gb = gigabyte * input_megabyte\n",
    "        return convert_gb\n",
    "    \n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    \n",
    "    print('model size: {:.3f} GB'.format(convert_to_gigabytes(size_all_mb)))\n",
    "    \n",
    "    return convert_to_gigabytes(size_all_mb)\n"
   ],
   "id": "1ab5f8eefd0e3b18",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "794267d4978c96f8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.simmtm.model import target_classifier\n",
    "\n",
    "from models.simmtm.config import Config\n",
    "\n",
    "def build_model(args, lr, configs, device='cuda', chkpoint=None):\n",
    "    \n",
    "    model = TFC(configs, args).to(device)\n",
    "    if chkpoint:\n",
    "        pretrained_dict = chkpoint[\"model_state_dict\"]\n",
    "        model_dict = model.state_dict()\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    classifier = target_classifier(configs).to(device)\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(configs.beta1, configs.beta2), weight_decay=0)\n",
    "    classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=lr, \n",
    "                                            betas=(configs.beta1, configs.beta2),\n",
    "                                            weight_decay=0)\n",
    "    model_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=model_optimizer, T_max=args.finetune_epoch)\n",
    "\n",
    "    return model, classifier, model_optimizer, classifier_optimizer, model_scheduler\n",
    "\n",
    "# model, classifier, model_optimizer, classifier_optimizer, model_scheduler = build_model(args, Config().lr, Config())\n"
   ],
   "id": "e77dd1109aeca00a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ea7f9946c0b9189d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score, \n",
    "                             precision_score, f1_score, recall_score)\n",
    "\n",
    "def model_finetune(model, val_dl, device, model_optimizer, model_scheduler, classifier=None, classifier_optimizer=None):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    total_auc = []\n",
    "    total_prc = []\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    outs = np.array([])\n",
    "    trgs = np.array([])\n",
    "\n",
    "    for data, labels in val_dl:\n",
    "        model_optimizer.zero_grad()\n",
    "        classifier_optimizer.zero_grad()\n",
    "\n",
    "        data, labels = data.float().to(device), labels.long().to(device)\n",
    "\n",
    "        # Produce embeddings\n",
    "        h, z = model(stage='train', x_in_t=data, pre_train=False)\n",
    "\n",
    "        # Add supervised classifier: 1) it's unique to finetuning. 2) this classifier will also be used in test\n",
    "        fea_concat = h\n",
    "\n",
    "        predictions = classifier(fea_concat)\n",
    "        fea_concat_flat = fea_concat.reshape(fea_concat.shape[0], -1)\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        print(predictions.shape, labels.shape)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        acc_bs = labels.eq(predictions.detach().argmax(dim=1)).float().mean()\n",
    "        onehot_label = F.one_hot(labels)\n",
    "        pred_numpy = predictions.detach().cpu().numpy()\n",
    "\n",
    "        try:\n",
    "            auc_bs = roc_auc_score(onehot_label.detach().cpu().numpy(), pred_numpy, average=\"macro\", multi_class=\"ovr\")\n",
    "        except:\n",
    "            auc_bs = 0.0\n",
    "\n",
    "        try:\n",
    "            prc_bs = average_precision_score(onehot_label.detach().cpu().numpy(), pred_numpy)\n",
    "        except:\n",
    "            prc_bs = 0.0\n",
    "\n",
    "        total_acc.append(acc_bs)\n",
    "\n",
    "        if auc_bs != 0:\n",
    "            total_auc.append(auc_bs)\n",
    "        if prc_bs != 0:\n",
    "            total_prc.append(prc_bs)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        pred = predictions.max(1, keepdim=True)[1]\n",
    "        outs = np.append(outs, pred.cpu().numpy())\n",
    "        trgs = np.append(trgs, labels.data.cpu().numpy())\n",
    "\n",
    "    labels_numpy = labels.detach().cpu().numpy()\n",
    "    pred_numpy = np.argmax(pred_numpy, axis=1)\n",
    "    F1 = f1_score(labels_numpy, pred_numpy, average='macro', )  # labels=np.unique(ypred))\n",
    "\n",
    "    total_loss = torch.tensor(total_loss).mean()  # average loss\n",
    "    total_acc = torch.tensor(total_acc).mean()  # average acc\n",
    "    total_auc = torch.tensor(total_auc).mean()  # average auc\n",
    "    total_prc = torch.tensor(total_prc).mean()\n",
    "\n",
    "    model_scheduler.step(total_loss)\n",
    "\n",
    "    return total_loss, total_acc, total_auc, total_prc, fea_concat_flat, trgs, F1\n"
   ],
   "id": "a094280591adf48d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b32855e4656dc272",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def train(train_loader, val_loader, finetune_loader, device='cuda'):\n",
    "    \n",
    "    model = TFC(configs=Config(), args=args)\n",
    "    params_group = [{'params': model.parameters()}]\n",
    "    model_optimizer = torch.optim.Adam(params_group, lr=args.pretrain_lr, \n",
    "                                       betas=(Config().beta1, Config().beta2),\n",
    "                                       weight_decay=0)\n",
    "    \n",
    "    model_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=model_optimizer, T_max=args.pretrain_epoch)\n",
    "\n",
    "    experiment_log_dir = os.path.join(project_root(), 'results', 'simmtm')\n",
    "    os.makedirs(os.path.join(experiment_log_dir, f\"saved_models\"), exist_ok=True)\n",
    "    \n",
    "    best_performance = None\n",
    "    seed = 2024\n",
    "    for epoch in range(Config().pretrain_epoch):\n",
    "        total_loss, total_cl_loss, total_rb_loss = model_pretrain(model=model, model_optimizer=model_optimizer,\n",
    "                                                              model_scheduler=model_scheduler, train_loader=train_loader, \n",
    "                                                              configs=Config(), args=args, device='cuda')\n",
    "        \n",
    "        print(f'Pre-training Epoch: {epoch}\\t Train Loss: {total_loss:.4f}\\t CL Loss: {total_cl_loss:.4f}\\t RB Loss: {total_rb_loss:.4f}\\n')\n",
    "        \n",
    "        chkpoint = {'seed': seed, 'epoch': epoch, 'train_loss': total_loss, 'model_state_dict': model.state_dict()}\n",
    "        torch.save(chkpoint, os.path.join(experiment_log_dir, f\"saved_models/\", f'ckp_ep{epoch}.pt'))\n",
    "        \n",
    "        # if epoch % 2 == 0:\n",
    "        for ep in range(1, Config().finetune_epoch+1):\n",
    "            print(f\"Fine-tuning started...\")\n",
    "            ft_model, ft_classifier, ft_model_optimizer, ft_classifier_optimizer, ft_scheduler = build_model(\n",
    "                args, args.lr, Config(), device=device, chkpoint=chkpoint)\n",
    "            \n",
    "            for ep in range(1, Config().finetune_epoch):\n",
    "                valid_loss, valid_acc, valid_auc, valid_prc, emb_finetune, label_finetune, F1 = model_finetune(\n",
    "                    ft_model, finetune_loader, device, ft_model_optimizer, ft_scheduler, classifier=ft_classifier,\n",
    "                    classifier_optimizer=ft_classifier_optimizer)\n",
    "        \n",
    "        \n",
    "        # # Loading the model\n",
    "        # temp_model = TFC(configs=Config(), args=args)\n",
    "        # \n",
    "        # pretrained_dict = chkpoint[\"model_state_dict\"]\n",
    "        # model_dict = temp_model.state_dict()\n",
    "        # model_dict.update(pretrained_dict)\n",
    "        # temp_model.load_state_dict(model_dict)\n",
    "\n",
    "train(train_loader, val_loader, finetune_loader)\n"
   ],
   "id": "1c828672c410828f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1f7e59217a9043",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e71b3bc07f12b1e6",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T02:51:27.643052Z",
     "start_time": "2024-09-15T02:51:27.639944Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "12d17b3c2d404d16",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T15:28:33.536951Z",
     "start_time": "2024-09-15T15:28:33.534768Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6f6fa66123ca65a2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:02:36.365948Z",
     "start_time": "2024-09-15T18:02:34.869333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.evaluate_helper_methods import load_sepsis_model\n",
    "from utils.path_utils import project_root\n",
    "import os\n",
    "\n",
    "model_path = os.path.join(project_root(), 'results', 'simmtm', 'saved_models', 'finetune_ep16.pt')\n",
    "model = load_sepsis_model(d_input=336, d_channel=40, d_output=2, model_name=model_path,\n",
    "                          pre_model=\"simmtm\")\n"
   ],
   "id": "d360583e1280b377",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:17:23.162317Z",
     "start_time": "2024-09-15T18:17:18.782160Z"
    }
   },
   "cell_type": "code",
   "source": "torch.load(model_path)['classifier']",
   "id": "79ca166a2a37eab",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "c4e9e917c22ff42e",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:22:52.533147Z",
     "start_time": "2024-09-15T18:22:52.460727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configs = Config()\n",
    "classifier = target_classifier(configs=configs)"
   ],
   "id": "961ffb366fa00050",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:22:57.821371Z",
     "start_time": "2024-09-15T18:22:57.818362Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6e8d8d3503885f",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T15:59:22.920520Z",
     "start_time": "2024-09-15T15:59:22.918701Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "595708fecee6ba5f",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:00:16.200591Z",
     "start_time": "2024-09-15T16:00:15.891235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "test_data = torch.load(os.path.join(project_root(), 'data', 'test_data', 'simmtm', 'test.pt'))['samples']\n"
   ],
   "id": "f48564a2eeed9fcf",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:06:24.652829Z",
     "start_time": "2024-09-15T16:06:24.650902Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "75e91fc0e75291de",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:06:52.411708Z",
     "start_time": "2024-09-15T16:06:52.398669Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "1d47b53b411323e4",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:08:44.721676Z",
     "start_time": "2024-09-15T16:08:44.708916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_setB_all_files = os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training',\n",
    "                         'training_setB')\n",
    "test_setB_files = os.listdir(test_setB_all_files)\n",
    "test_setB_files.sort()\n",
    "test_setB_files.remove('index.html')\n",
    "\n",
    "test_setB_files = [test_setB_files[i] for i in test_indices]\n"
   ],
   "id": "a7c2ccc8e4d4b26",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:16:05.258640Z",
     "start_time": "2024-09-15T16:15:35.191282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_path = os.path.join(project_root(), 'data', 'test_data', 'simmtm', 'psv_files')\n",
    "for pidx in test_setB_files:\n",
    "    pdata = pd.read_csv(os.path.join(test_setB_all_files, pidx), sep='|')\n",
    "    pdata.to_csv(os.path.join(save_path, pidx), sep='|', index=False)\n"
   ],
   "id": "4c401801fd054fc2",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:16:05.261393Z",
     "start_time": "2024-09-15T16:16:05.259803Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d9210aa2d11c0419",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T17:06:28.556415Z",
     "start_time": "2024-09-16T17:06:23.059358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils.path_utils import project_root\n",
    "from utils.pretrain_utils.data import get_train_val_test_indices\n",
    "\n",
    "test_indices, finetune_indices = get_train_val_test_indices(\n",
    "        sepsis_file='is_sepsis_finetune_B.txt', save_distributions=True,\n",
    "        dset='Bb')\n",
    "\n",
    "test_setB_all_files = os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training',\n",
    "                         'training_setB')\n",
    "test_setB_files = os.listdir(test_setB_all_files)\n",
    "test_setB_files.sort()\n",
    "test_setB_files.remove('index.html')\n",
    "\n",
    "test_setB_files = [test_setB_files[i] for i in test_indices]\n",
    "\n",
    "for pidx in test_setB_files:\n",
    "    break\n",
    "    \n",
    "save_path = os.path.join(project_root(), 'data', 'test_data', 'simmtm', 'psv_files')\n",
    "pd.read_csv(os.path.join(save_path, pidx), sep='|')"
   ],
   "id": "755d73d5d746dd79",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:15:31.666688Z",
     "start_time": "2024-09-15T16:15:31.665040Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6fdb9e3b02c4d350",
   "execution_count": 66,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:48:30.348775Z",
     "start_time": "2024-09-15T16:48:30.346546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# torch.load(os.path.join(project_root(), 'results', 'simmtm', 'saved_models', 'ckp_ep9.pt'))['model_state_dict']"
   ],
   "id": "ac9ec8ab87a8be4c",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:49:09.393511Z",
     "start_time": "2024-09-15T16:49:09.254354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.simmtm.config import Config\n",
    "from models.simmtm.model import TFC, target_classifier\n",
    "\n",
    "config = Config()\n",
    "classifier = target_classifier(config).to('cuda')\n"
   ],
   "id": "d9acb3ab57655c84",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:49:43.678359Z",
     "start_time": "2024-09-15T16:49:43.676184Z"
    }
   },
   "cell_type": "code",
   "source": "[8, 15 (0.89), 16 (1), 17 (1)]",
   "id": "6d2f46245e123e90",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "134ec7bccf2a316f",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
