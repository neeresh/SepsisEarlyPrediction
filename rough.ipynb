{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-15T07:21:16.528424Z",
     "start_time": "2024-09-15T07:21:16.526913Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:21:19.999998Z",
     "start_time": "2024-09-15T07:21:16.538936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from utils.path_utils import project_root\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n"
   ],
   "id": "e26363a27c9bea16",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:21:20.002992Z",
     "start_time": "2024-09-15T07:21:20.001272Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "35310e36b03c6b19",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:35:31.802255Z",
     "start_time": "2024-09-15T06:34:08.242344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.pretrain_utils.data import get_pretrain_finetune_test_datasets\n",
    "\n",
    "pt_train, finetune, test = get_pretrain_finetune_test_datasets()"
   ],
   "id": "9a29c339fca5d558",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking Pre-training & Validation Files: 100%|██████████| 20336/20336 [00:19<00:00, 1040.31it/s]\n",
      "PT Train Set: 100%|██████████| 20336/20336 [00:07<00:00, 2664.19it/s]\n",
      "Checking Fine-tuning & Test Files: 100%|██████████| 20000/20000 [00:21<00:00, 917.84it/s] \n",
      "Fine-tuning Set: 100%|██████████| 4000/4000 [00:00<00:00, 4108.02it/s]\n",
      "Test Set: 100%|██████████| 16000/16000 [00:04<00:00, 3932.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training samples:  torch.Size([20336, 336, 40])\n",
      "Fine-tuning samples:  torch.Size([4000, 336, 40]) Test samples:  torch.Size([16000, 336, 40])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:37:22.441190Z",
     "start_time": "2024-09-15T06:37:22.439357Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9b37998e87bf5872",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:49:52.473939Z",
     "start_time": "2024-09-15T06:49:52.472098Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5233f8fc26cbe6fb",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T06:40:11.194256Z",
     "start_time": "2024-09-15T06:40:11.192537Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e2a43b014aa8152d",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:34:22.390496Z",
     "start_time": "2024-09-15T18:34:22.065201Z"
    }
   },
   "cell_type": "code",
   "source": "!sbatch eval_simmtm.job",
   "id": "2f58dbfac2ed3df3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 24928278\r\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T02:06:47.919688Z",
     "start_time": "2024-09-15T02:06:47.917946Z"
    }
   },
   "cell_type": "code",
   "source": "-",
   "id": "26f22e2c11ae962d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "711daf7b9fb53cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Args",
   "id": "f4d653dde45d4dc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "parser.add_argument('--run_description', default='run1', type=str, help='Experiment Description')\n",
    "parser.add_argument('--seed', default=2023, type=int, help='seed value')\n",
    "\n",
    "parser.add_argument('--training_mode', default='pre_train', type=str, help='pre_train, fine_tune')\n",
    "parser.add_argument('--pretrain_dataset', default='SleepEEG', type=str,\n",
    "                    help='Dataset of choice: SleepEEG, FD_A, HAR, ECG')\n",
    "parser.add_argument('--target_dataset', default='Epilepsy', type=str,\n",
    "                    help='Dataset of choice: Epilepsy, FD_B, Gesture, EMG')\n",
    "\n",
    "parser.add_argument('--logs_save_dir', default='experiments_logs', type=str, help='saving directory')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='cpu or cuda')\n",
    "parser.add_argument('--home_path', default=home_dir, type=str, help='Project home directory')\n",
    "parser.add_argument('--subset', action='store_true', default=False, help='use the subset of datasets')\n",
    "parser.add_argument('--log_epoch', default=5, type=int, help='print loss and metrix')\n",
    "parser.add_argument('--draw_similar_matrix', default=10, type=int, help='draw similarity matrix')\n",
    "parser.add_argument('--pretrain_lr', default=0.0001, type=float, help='pretrain learning rate')\n",
    "parser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
    "parser.add_argument('--use_pretrain_epoch_dir', default=None, type=str,\n",
    "                    help='choose the pretrain checkpoint to finetune')\n",
    "parser.add_argument('--pretrain_epoch', default=10, type=int, help='pretrain epochs')\n",
    "parser.add_argument('--finetune_epoch', default=300, type=int, help='finetune epochs')\n",
    "\n",
    "parser.add_argument('--masking_ratio', default=0.5, type=float, help='masking ratio')\n",
    "parser.add_argument('--positive_nums', default=3, type=int, help='positive series numbers')\n",
    "parser.add_argument('--lm', default=3, type=int, help='average masked lenght')\n",
    "\n",
    "parser.add_argument('--finetune_result_file_name', default=\"finetune_result.json\", type=str,\n",
    "                    help='finetune result json name')\n",
    "parser.add_argument('--temperature', type=float, default=0.2, help='temperature')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ],
   "id": "8f0f9109afd5d6f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f43119c437903e8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert csv to pt",
   "id": "2bae8b6156fcaa81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def csv_to_pt(patient_files, lengths, is_sepsis, desc):\n",
    "    \n",
    "    all_patients = {'samples': [], 'labels': []}\n",
    "    \n",
    "    max_time_step = 336\n",
    "    # print(len(patient_files), len(lengths), len(is_sepsis))\n",
    "    for idx, (file, length, sepsis) in tqdm.tqdm(enumerate(zip(patient_files, lengths, is_sepsis)), \n",
    "                                                      desc=f\"{desc}\", \n",
    "                                                      total=len(patient_files)):\n",
    "        \n",
    "        pad_width = ((0, max_time_step - len(file)), (0, 0))\n",
    "        file = np.pad(file, pad_width=pad_width, mode='constant').astype(np.float32)\n",
    "        \n",
    "        if len(file) == max_time_step:\n",
    "            all_patients['samples'].append(torch.from_numpy(file).unsqueeze(0))\n",
    "            all_patients['labels'].append(torch.tensor(sepsis, dtype=torch.float32).unsqueeze(0))\n",
    "        else:\n",
    "            raise ValueError(f\"Length {length} does not match length of patient {idx} with length {len(file)}\")\n",
    "    \n",
    "    # print('samples: ', type(all_patients['samples']), 'labels: ', type(all_patients['labels']))\n",
    "    \n",
    "    all_patients['samples'] = torch.cat(all_patients['samples'], dim=0)\n",
    "    all_patients['labels'] = torch.cat(all_patients['labels'], dim=0)\n",
    "    \n",
    "    return {'samples': all_patients['samples'], 'labels': all_patients['labels']}, lengths, is_sepsis\n",
    "\n",
    "# all_patients, lengths, is_sepsis = csv_to_pt()\n"
   ],
   "id": "123cbaf79652006c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "43753974148ee5d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_val_test_indices(sepsis_file, dset, save_distributions=True):\n",
    "    \n",
    "    sepsis = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', f'{sepsis_file}'), header=None)\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(sepsis, test_size=0.2, random_state=2024)\n",
    "    \n",
    "    train_indices = train_indices.index.values\n",
    "    val_indices = val_indices.index.values\n",
    "    \n",
    "    \n",
    "    if save_distributions:\n",
    "        train_dist = sepsis.iloc[train_indices].value_counts()\n",
    "        val_dist = sepsis.iloc[val_indices].value_counts()\n",
    "        \n",
    "        train_dist_percentage = np.round(train_dist / len(sepsis.iloc[train_indices]), 2)\n",
    "        val_dist_percentage = np.round(val_dist / len(sepsis.iloc[val_indices]), 2)\n",
    "        \n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                'Train Images': train_dist, 'Validation Images': val_dist,\n",
    "                'Train Distribution Percentage': train_dist_percentage, 'Validation Distribution Percentage': val_dist_percentage,\n",
    "            }\n",
    "        ).to_csv(os.path.join(project_root(), 'results', f'distributions{dset}.csv'), index=False)\n",
    "        \n",
    "        # pd.read_csv(os.path.join(project_root(), 'results', 'distributions.csv'))\n",
    "        \n",
    "    return train_indices, val_indices\n"
   ],
   "id": "717b9505d28c8ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "10aee72c9dbf0a4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasetup",
   "id": "fbb5c03280f398db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_pretrain_finetune_datasets():\n",
    "    \n",
    "    # Pre-training Indices\n",
    "    pt_train_indices, pt_val_indices = get_train_val_test_indices(\n",
    "        sepsis_file='is_sepsis_pretrain_A.txt', save_distributions=True,\n",
    "        dset='Aa')\n",
    "    \n",
    "    # Gathering files, lengths, and sepsis label\n",
    "    pt_files = pd.read_pickle(os.path.join(project_root(), 'data', 'tl_datasets', 'final_dataset_pretrain_A.pickle'))\n",
    "    pt_lengths = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'lengths_pretrain_A.txt'), \n",
    "                             header=None)\n",
    "    pt_sepsis = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'is_sepsis_pretrain_A.txt'),\n",
    "                            header=None)\n",
    "    \n",
    "    # Checking whether the files are in same order or not\n",
    "    pretrain_files = []\n",
    "    for pdata, length in tqdm.tqdm(zip(pt_files, pt_lengths.values), desc=\"Checking Pre-training & Validation Files\", \n",
    "                                   total=len(pt_files)):\n",
    "        plength = len(pdata) \n",
    "        assert plength == length[0], f\"{plength} doesn't match {length}\"\n",
    "        pretrain_files.append(pdata.drop(['PatientID', 'SepsisLabel'], axis=1))\n",
    "    \n",
    "    # Getting train and val\n",
    "    pt_train = [pretrain_files[i] for i in pt_train_indices]\n",
    "    pt_val = [pretrain_files[i] for i in pt_val_indices]\n",
    "    \n",
    "    pt_train_lengths = pt_lengths.iloc[pt_train_indices].values\n",
    "    pt_val_lengths = pt_lengths.iloc[pt_val_indices].values\n",
    "    \n",
    "    pt_train_sepsis = pt_sepsis.iloc[pt_train_indices].values\n",
    "    pt_val_sepsis = pt_sepsis.iloc[pt_val_indices].values\n",
    "    \n",
    "    pt_train, pt_train_lengths, pt_train_sepsis = csv_to_pt(pt_train, pt_train_lengths, pt_train_sepsis, desc='PT Train Set')\n",
    "    pt_val, pt_val_lengths, pt_val_sepsis = csv_to_pt(pt_val, pt_val_lengths, pt_val_sepsis, desc='PT Validation Set')\n",
    "    \n",
    "    # Fine-tuning\n",
    "    test_indices, finetune_indices = get_train_val_test_indices(\n",
    "        sepsis_file='is_sepsis_finetune_B.txt', save_distributions=True,\n",
    "        dset='Bb')\n",
    "    \n",
    "    # Gathering files, lengths, and sepsis label\n",
    "    test_setB = pd.read_pickle(os.path.join(project_root(), 'data', 'tl_datasets', 'final_dataset_finetune_B.pickle'))\n",
    "    test_setB_lengths = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'lengths_finetune_B.txt'), \n",
    "                             header=None)\n",
    "    test_setB_sepsis = pd.read_csv(os.path.join(project_root(), 'data', 'tl_datasets', 'is_sepsis_finetune_B.txt'),\n",
    "                            header=None)\n",
    "    \n",
    "    # Checking whether the files are in same order or not\n",
    "    test_files = []\n",
    "    for pdata, length in tqdm.tqdm(zip(test_setB, test_setB_lengths.values), desc=\"Checking Fine-tuning & Test Files\",\n",
    "                                   total=len(test_setB)):\n",
    "        plength = len(pdata) \n",
    "        assert plength == length[0], f\"{plength} doesn't match {length}\"\n",
    "        test_files.append(pdata.drop(['PatientID', 'SepsisLabel'], axis=1))\n",
    "    \n",
    "    # Getting finetune and test sets\n",
    "    finetune = [test_files[i] for i in finetune_indices]\n",
    "    test = [test_files[i] for i in test_indices]\n",
    "    \n",
    "    finetune_lengths = test_setB_lengths.iloc[finetune_indices].values\n",
    "    test_lengths = test_setB_lengths.iloc[test_indices].values\n",
    "    \n",
    "    finetune_sepsis = test_setB_sepsis.iloc[finetune_indices].values\n",
    "    test_sepsis = test_setB_sepsis.iloc[test_indices].values\n",
    "    \n",
    "    finetune, finetune_lengths, finetune_sepsis = csv_to_pt(finetune, finetune_lengths, finetune_sepsis, desc=\"Fine-tuning Set\")\n",
    "    test, test_lengths, test_sepsis = csv_to_pt(test, test_lengths, test_sepsis, desc=\"Test Set\")\n",
    "    \n",
    "    print(\"Pre-training samples: \", pt_train['samples'].shape, \"Validation samples: \", pt_val['samples'].shape)\n",
    "    print(\"Fine-tuning samples: \", finetune['samples'].shape, \"Test samples: \", test['samples'].shape)\n",
    "    \n",
    "    return pt_train, pt_val, finetune, test\n",
    "    \n",
    "pt_train, pt_val, finetune, test = get_pretrain_finetune_datasets()\n"
   ],
   "id": "687ad37dc749028d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b8503e477808e77e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "    Returns:\n",
    "        (L, ) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (\n",
    "            1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask\n",
    "\n",
    "\n",
    "def noise_mask(X, masking_ratio=0.25, lm=3, distribution='geometric', exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n",
    "        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n",
    "            feat_dim that will be masked on average\n",
    "        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n",
    "        distribution: whether each mask sequence element is sampled independently at random, or whether\n",
    "            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n",
    "            masked squences of a desired mean length `lm`\n",
    "        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n",
    "    Returns:\n",
    "        boolean numpy array with the same shape as X, with 0s at places where a feature should be masked\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        mask = geom_noise_mask_single(X.shape[0] * X.shape[1] * X.shape[2], lm, masking_ratio)\n",
    "        mask = mask.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "        \n",
    "    elif distribution == 'masked_tail':\n",
    "        mask = np.ones(X.shape, dtype=bool)\n",
    "        for m in range(X.shape[0]):  # feature dimension\n",
    "\n",
    "            keep_mask = np.zeros_like(mask[m, :], dtype=bool)\n",
    "            n = math.ceil(keep_mask.shape[1] * (1 - masking_ratio))\n",
    "            keep_mask[:, :n] = True\n",
    "            mask[m, :] = keep_mask  # time dimension\n",
    "            \n",
    "    elif distribution == 'masked_head':\n",
    "        mask = np.ones(X.shape, dtype=bool)\n",
    "        for m in range(X.shape[0]):  # feature dimension\n",
    "\n",
    "            keep_mask = np.zeros_like(mask[m, :], dtype=bool)\n",
    "            n = math.ceil(keep_mask.shape[1] * masking_ratio)\n",
    "            keep_mask[:, n:] = True\n",
    "            mask[m, :] = keep_mask  # time dimension\n",
    "    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n",
    "        mask = np.random.choice(np.array([True, False]), size=X.shape, replace=True,\n",
    "                                p=(1 - masking_ratio, masking_ratio))\n",
    "\n",
    "    return torch.tensor(mask)\n",
    "\n",
    "def data_transform_masked4cl(sample, masking_ratio, lm, positive_nums=None, distribution='geometric'):\n",
    "    \"\"\"Masked time series in time dimension\"\"\"\n",
    "\n",
    "    if positive_nums is None:\n",
    "        positive_nums = math.ceil(1.5 / (1 - masking_ratio))\n",
    "        \n",
    "    sample = sample.permute(0, 2, 1)  # (batch_size, channels, time_steps)\n",
    "    \n",
    "    # Creating the batch in #positive_nums sets\n",
    "    sample_repeat = sample.repeat(positive_nums, 1, 1)  # (batch_size*positive_num, channels, time steps)\n",
    "\n",
    "    mask = noise_mask(sample_repeat, masking_ratio, lm, distribution=distribution)\n",
    "    x_masked = mask * sample_repeat\n",
    "\n",
    "    return x_masked.permute(0, 2, 1), mask.permute(0, 2, 1)\n",
    "\n",
    "# data_masked_m, mask = data_transform_masked4cl(all_patients['samples'][:32], 0.5, 3, positive_nums=1, distribution='geometric')\n"
   ],
   "id": "df2ee9f255cd3528",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e14802c72ae0ed5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Load_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, TSlength_aligned, training_mode):\n",
    "        \n",
    "        super(Load_Dataset, self).__init__()\n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        X_train = dataset[\"samples\"]\n",
    "        y_train = dataset[\"labels\"]\n",
    "        \n",
    "        # shuffle\n",
    "        data = list(zip(X_train, y_train))\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        X_train, y_train = zip(*data)\n",
    "        X_train, y_train = torch.stack(list(X_train), dim=0), torch.stack(list(y_train), dim=0)\n",
    "\n",
    "        if len(X_train.shape) < 3:\n",
    "            X_train = X_train.unsqueeze(2)\n",
    "\n",
    "        # if X_train.shape.index(min(X_train.shape)) != 1:  # make sure the Channels in second dim\n",
    "        #     X_train = X_train.permute(0, 2, 1)\n",
    "\n",
    "        \"\"\"Align the TS length between source and target datasets\"\"\"\n",
    "        # X_train = X_train[:, :1, :int(config.TSlength_aligned)] # take the first 178 samples\n",
    "        X_train = X_train[:, :, :int(TSlength_aligned)]\n",
    "        \n",
    "        if isinstance(X_train, np.ndarray):\n",
    "            self.x_data = torch.from_numpy(X_train)\n",
    "            self.y_data = torch.from_numpy(y_train).long()\n",
    "        else:\n",
    "            self.x_data = X_train\n",
    "            self.y_data = y_train\n",
    "\n",
    "        self.len = X_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ],
   "id": "813e977f40d98345",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1a86d99d645f058",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.simmtm.gtn.encoder import Encoder\n",
    "from simmtm.loss import ContrastiveWeight, AggregationRebuild, AutomaticWeightedLoss\n",
    "\n",
    "\n",
    "class TFC(nn.Module):\n",
    "    def __init__(self, configs, args):\n",
    "        \n",
    "        super(TFC, self).__init__()\n",
    "        self.training_mode = 'pre_train'\n",
    "        \n",
    "        # Projecting input into deep representations\n",
    "        self.encoder_list_1 = ModuleList([Encoder(d_model=configs.d_model, d_hidden=configs.d_hidden, q=configs.q,\n",
    "                                                  v=configs.v, h=configs.h, mask=configs.mask, dropout=configs.dropout,\n",
    "                                                  device=configs.device) for _ in range(configs.N)])\n",
    "\n",
    "        self.encoder_list_2 = ModuleList([Encoder(d_model=configs.d_model, d_hidden=configs.d_hidden, q=configs.q,\n",
    "                                                  v=configs.v, h=configs.h, dropout=configs.dropout,\n",
    "                                                  device=configs.device) for _ in range(configs.N)])\n",
    "\n",
    "        self.embedding_channel = torch.nn.Linear(configs.d_channel, configs.d_model)\n",
    "        self.embedding_input = torch.nn.Linear(configs.d_input, configs.d_model)\n",
    "\n",
    "        self.gate = torch.nn.Linear(configs.d_model * configs.d_input + configs.d_model * configs.d_channel,\n",
    "                                    configs.d_output)\n",
    "\n",
    "        self.pe = configs.pe\n",
    "        self._d_input = configs.d_input\n",
    "        self._d_model = configs.d_model\n",
    "\n",
    "        # MLP Layer - To generate Projector(.); to Obtain series-wise representations\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(192512, 256),  # 240128 = encoder1 out features + encoder2 out features\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "        if self.training_mode == 'pre_train':\n",
    "            self.awl = AutomaticWeightedLoss(2)\n",
    "            self.contrastive = ContrastiveWeight(args)\n",
    "            self.aggregation = AggregationRebuild(args)\n",
    "            # self.head = nn.Linear(240128, 336)  # Reconstruction, we have 336 time steps\n",
    "            self.head = nn.Linear(192512, configs.d_input * configs.d_channel)  # Replaced to handle multi-variate\n",
    "            self.mse = torch.nn.MSELoss()\n",
    "            \n",
    "    def forward(self, stage, x_in_t, pre_train=False):\n",
    "\n",
    "        # x_in_t: (128, 336, 133)\n",
    "        encoding_1 = self.embedding_channel(x_in_t)  # (128, 336, 512)\n",
    "        input_to_gather = encoding_1 \n",
    "\n",
    "        if self.pe:\n",
    "            pe = torch.ones_like(encoding_1[0])\n",
    "            position = torch.arange(0, self._d_input).unsqueeze(-1)\n",
    "            temp = torch.Tensor(range(0, self._d_model, 2))\n",
    "            temp = temp * -(math.log(10000) / self._d_model)\n",
    "            temp = torch.exp(temp).unsqueeze(0)\n",
    "            temp = torch.matmul(position.float(), temp)  # shape:[input, d_model/2]\n",
    "            pe[:, 0::2] = torch.sin(temp)\n",
    "            pe[:, 1::2] = torch.cos(temp)\n",
    "\n",
    "            encoding_1 = encoding_1 + pe  # (128, 336, 512)\n",
    "\n",
    "        for encoder in self.encoder_list_1:\n",
    "            # encoding_1: (128, 336, 512)\n",
    "            encoding_1, score_input = encoder(encoding_1, stage)\n",
    "            \n",
    "        encoding_2 = self.embedding_input(x_in_t.transpose(-1, -2))  # encoding_2: (128, 133, 512)\n",
    "        channel_to_gather = encoding_2  \n",
    "\n",
    "        for encoder in self.encoder_list_2:\n",
    "            # encoding_2: (128, 133, 512)\n",
    "            encoding_2, score_channel = encoder(encoding_2, stage)\n",
    "\n",
    "        encoding_1 = encoding_1.reshape(encoding_1.shape[0], -1)  # (128, 172032)\n",
    "        encoding_2 = encoding_2.reshape(encoding_2.shape[0], -1)  # (128, 68096)\n",
    "        \n",
    "        encoding_concat = self.gate(torch.cat([encoding_1, encoding_2], dim=-1))  # (128, 2)\n",
    "        \n",
    "        # gate: torch.Size([128, 2])\n",
    "        gate = F.softmax(encoding_concat, dim=-1)  \n",
    "        encoding = torch.cat([encoding_1 * gate[:, 0:1], encoding_2 * gate[:, 1:2]], dim=-1)  # (128, 240128)\n",
    "        # print(encoding.shape)\n",
    "        \n",
    "        # Projections\n",
    "        projections = self.dense(encoding)  # (128, 128)\n",
    "\n",
    "        if pre_train:\n",
    "            # loss_cl: torch.Size([])\n",
    "            # similarity_matrix: torch.Size([128, 128])\n",
    "            # logits: torch.Size([128, 127])\n",
    "            # positives_mask: torch.Size([128, 128])\n",
    "            loss_cl, similarity_matrix, logits, positives_mask = self.contrastive(projections)           \n",
    "            \n",
    "            # rebuild_weight_matrix: torch.Size([128, 128])\n",
    "            # agg_x: torch.Size([128, 240128])\n",
    "            rebuild_weight_matrix, agg_x = self.aggregation(similarity_matrix, encoding)\n",
    "            \n",
    "            # pred_x: torch.Size([128, 336])\n",
    "            pred_x = self.head(agg_x.reshape(agg_x.size(0), -1))\n",
    "            \n",
    "            # x_in_t.shape: torch.Size([128, 336, 133])\n",
    "            # x_in_t.reshape(x_in_t.size(0), -1): torch.Size([128, 44688])\n",
    "            loss_rb = self.mse(pred_x, x_in_t.reshape(x_in_t.size(0), -1).detach())\n",
    "            loss = self.awl(loss_cl, loss_rb)\n",
    "\n",
    "            return loss, loss_cl, loss_rb\n",
    "\n",
    "        return encoding, encoding_concat\n",
    "    "
   ],
   "id": "5df8708066259139",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f16d7cef8d8db61f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def model_pretrain(model, model_optimizer, model_scheduler, train_loader, configs, args, device):\n",
    "    total_loss = []\n",
    "    total_cl_loss = []\n",
    "    total_rb_loss = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in tqdm.tqdm(enumerate(train_loader), desc=\"Pre-training model\", total=len(train_loader)):  # data shape: (batch_size, seqs, channels)\n",
    "\n",
    "        model_optimizer.zero_grad()\n",
    "        # When masking, data is reshaped to (batch_size, channel, seqs) - Inside the data_transform_masked4cl()\n",
    "        data_masked_m, mask = data_transform_masked4cl(data, args.masking_ratio, args.lm, args.positive_nums)\n",
    "        data_masked_om = torch.cat([data, data_masked_m], 0)  # (batch_size, seqs, channels)\n",
    "\n",
    "        data, labels = data.float().to('cpu'), labels.float().to('cpu')\n",
    "        data_masked_om = data_masked_om.float().to(device)\n",
    "\n",
    "        # Produce embeddings of original and masked samples  (data_masked_om = data samples + masked samples)\n",
    "        # loss, loss_cl, loss_rb = model(data_masked_om, pretrain=True)\n",
    "        # return loss, loss_cl, loss_rb\n",
    "        \n",
    "        loss, loss_cl, loss_rb = model(stage='train', x_in_t=data_masked_om, pre_train=True)\n",
    "        \n",
    "        # return loss, loss_cl, loss_rb\n",
    "\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        total_cl_loss.append(loss_cl.item())\n",
    "        total_rb_loss.append(loss_rb.item())\n",
    "\n",
    "    total_loss = torch.tensor(total_loss).mean()\n",
    "    total_cl_loss = torch.tensor(total_cl_loss).mean()\n",
    "    total_rb_loss = torch.tensor(total_rb_loss).mean()\n",
    "\n",
    "    model_scheduler.step()\n",
    "\n",
    "    return total_loss, total_cl_loss, total_rb_loss\n"
   ],
   "id": "53dd90f7b8f1d93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70fb02e9daf22921",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pt_dataset = Load_Dataset(pt_train, TSlength_aligned=336, training_mode='pretrain')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=pt_dataset, batch_size=32, shuffle=True, \n",
    "                                           drop_last=True, num_workers=4)  # (32, 336, 40)\n",
    "\n",
    "val_dataset = Load_Dataset(pt_val, TSlength_aligned=336, training_mode='pretrain')\n",
    "val_loader = torch.utils.data.DataLoader(dataset=pt_val, batch_size=32, shuffle=True, \n",
    "                                           drop_last=True, num_workers=4)\n",
    "\n",
    "finetune_dataset = Load_Dataset(finetune, TSlength_aligned=336, training_mode='finetune')\n",
    "finetune_loader = torch.utils.data.DataLoader(finetune_dataset, batch_size=32, shuffle=True, \n",
    "                                              drop_last=True, num_workers=4)\n",
    "\n"
   ],
   "id": "73cf01b5a50301d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4791a78dc05d8fbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "66c8d4e9473a73ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_model_size(model):\n",
    "    \n",
    "    def convert_to_gigabytes(input_megabyte):\n",
    "        gigabyte = 1.0/1024\n",
    "        convert_gb = gigabyte * input_megabyte\n",
    "        return convert_gb\n",
    "    \n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    \n",
    "    print('model size: {:.3f} GB'.format(convert_to_gigabytes(size_all_mb)))\n",
    "    \n",
    "    return convert_to_gigabytes(size_all_mb)\n"
   ],
   "id": "1ab5f8eefd0e3b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "794267d4978c96f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.simmtm.model import target_classifier\n",
    "\n",
    "from models.simmtm.config import Config\n",
    "\n",
    "def build_model(args, lr, configs, device='cuda', chkpoint=None):\n",
    "    \n",
    "    model = TFC(configs, args).to(device)\n",
    "    if chkpoint:\n",
    "        pretrained_dict = chkpoint[\"model_state_dict\"]\n",
    "        model_dict = model.state_dict()\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    classifier = target_classifier(configs).to(device)\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(configs.beta1, configs.beta2), weight_decay=0)\n",
    "    classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=lr, \n",
    "                                            betas=(configs.beta1, configs.beta2),\n",
    "                                            weight_decay=0)\n",
    "    model_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=model_optimizer, T_max=args.finetune_epoch)\n",
    "\n",
    "    return model, classifier, model_optimizer, classifier_optimizer, model_scheduler\n",
    "\n",
    "# model, classifier, model_optimizer, classifier_optimizer, model_scheduler = build_model(args, Config().lr, Config())\n"
   ],
   "id": "e77dd1109aeca00a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ea7f9946c0b9189d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score, \n",
    "                             precision_score, f1_score, recall_score)\n",
    "\n",
    "def model_finetune(model, val_dl, device, model_optimizer, model_scheduler, classifier=None, classifier_optimizer=None):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    total_auc = []\n",
    "    total_prc = []\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    outs = np.array([])\n",
    "    trgs = np.array([])\n",
    "\n",
    "    for data, labels in val_dl:\n",
    "        model_optimizer.zero_grad()\n",
    "        classifier_optimizer.zero_grad()\n",
    "\n",
    "        data, labels = data.float().to(device), labels.long().to(device)\n",
    "\n",
    "        # Produce embeddings\n",
    "        h, z = model(stage='train', x_in_t=data, pre_train=False)\n",
    "\n",
    "        # Add supervised classifier: 1) it's unique to finetuning. 2) this classifier will also be used in test\n",
    "        fea_concat = h\n",
    "\n",
    "        predictions = classifier(fea_concat)\n",
    "        fea_concat_flat = fea_concat.reshape(fea_concat.shape[0], -1)\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        print(predictions.shape, labels.shape)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        acc_bs = labels.eq(predictions.detach().argmax(dim=1)).float().mean()\n",
    "        onehot_label = F.one_hot(labels)\n",
    "        pred_numpy = predictions.detach().cpu().numpy()\n",
    "\n",
    "        try:\n",
    "            auc_bs = roc_auc_score(onehot_label.detach().cpu().numpy(), pred_numpy, average=\"macro\", multi_class=\"ovr\")\n",
    "        except:\n",
    "            auc_bs = 0.0\n",
    "\n",
    "        try:\n",
    "            prc_bs = average_precision_score(onehot_label.detach().cpu().numpy(), pred_numpy)\n",
    "        except:\n",
    "            prc_bs = 0.0\n",
    "\n",
    "        total_acc.append(acc_bs)\n",
    "\n",
    "        if auc_bs != 0:\n",
    "            total_auc.append(auc_bs)\n",
    "        if prc_bs != 0:\n",
    "            total_prc.append(prc_bs)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        pred = predictions.max(1, keepdim=True)[1]\n",
    "        outs = np.append(outs, pred.cpu().numpy())\n",
    "        trgs = np.append(trgs, labels.data.cpu().numpy())\n",
    "\n",
    "    labels_numpy = labels.detach().cpu().numpy()\n",
    "    pred_numpy = np.argmax(pred_numpy, axis=1)\n",
    "    F1 = f1_score(labels_numpy, pred_numpy, average='macro', )  # labels=np.unique(ypred))\n",
    "\n",
    "    total_loss = torch.tensor(total_loss).mean()  # average loss\n",
    "    total_acc = torch.tensor(total_acc).mean()  # average acc\n",
    "    total_auc = torch.tensor(total_auc).mean()  # average auc\n",
    "    total_prc = torch.tensor(total_prc).mean()\n",
    "\n",
    "    model_scheduler.step(total_loss)\n",
    "\n",
    "    return total_loss, total_acc, total_auc, total_prc, fea_concat_flat, trgs, F1\n"
   ],
   "id": "a094280591adf48d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b32855e4656dc272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def train(train_loader, val_loader, finetune_loader, device='cuda'):\n",
    "    \n",
    "    model = TFC(configs=Config(), args=args)\n",
    "    params_group = [{'params': model.parameters()}]\n",
    "    model_optimizer = torch.optim.Adam(params_group, lr=args.pretrain_lr, \n",
    "                                       betas=(Config().beta1, Config().beta2),\n",
    "                                       weight_decay=0)\n",
    "    \n",
    "    model_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=model_optimizer, T_max=args.pretrain_epoch)\n",
    "\n",
    "    experiment_log_dir = os.path.join(project_root(), 'results', 'simmtm')\n",
    "    os.makedirs(os.path.join(experiment_log_dir, f\"saved_models\"), exist_ok=True)\n",
    "    \n",
    "    best_performance = None\n",
    "    seed = 2024\n",
    "    for epoch in range(Config().pretrain_epoch):\n",
    "        total_loss, total_cl_loss, total_rb_loss = model_pretrain(model=model, model_optimizer=model_optimizer,\n",
    "                                                              model_scheduler=model_scheduler, train_loader=train_loader, \n",
    "                                                              configs=Config(), args=args, device='cuda')\n",
    "        \n",
    "        print(f'Pre-training Epoch: {epoch}\\t Train Loss: {total_loss:.4f}\\t CL Loss: {total_cl_loss:.4f}\\t RB Loss: {total_rb_loss:.4f}\\n')\n",
    "        \n",
    "        chkpoint = {'seed': seed, 'epoch': epoch, 'train_loss': total_loss, 'model_state_dict': model.state_dict()}\n",
    "        torch.save(chkpoint, os.path.join(experiment_log_dir, f\"saved_models/\", f'ckp_ep{epoch}.pt'))\n",
    "        \n",
    "        # if epoch % 2 == 0:\n",
    "        for ep in range(1, Config().finetune_epoch+1):\n",
    "            print(f\"Fine-tuning started...\")\n",
    "            ft_model, ft_classifier, ft_model_optimizer, ft_classifier_optimizer, ft_scheduler = build_model(\n",
    "                args, args.lr, Config(), device=device, chkpoint=chkpoint)\n",
    "            \n",
    "            for ep in range(1, Config().finetune_epoch):\n",
    "                valid_loss, valid_acc, valid_auc, valid_prc, emb_finetune, label_finetune, F1 = model_finetune(\n",
    "                    ft_model, finetune_loader, device, ft_model_optimizer, ft_scheduler, classifier=ft_classifier,\n",
    "                    classifier_optimizer=ft_classifier_optimizer)\n",
    "        \n",
    "        \n",
    "        # # Loading the model\n",
    "        # temp_model = TFC(configs=Config(), args=args)\n",
    "        # \n",
    "        # pretrained_dict = chkpoint[\"model_state_dict\"]\n",
    "        # model_dict = temp_model.state_dict()\n",
    "        # model_dict.update(pretrained_dict)\n",
    "        # temp_model.load_state_dict(model_dict)\n",
    "\n",
    "train(train_loader, val_loader, finetune_loader)\n"
   ],
   "id": "1c828672c410828f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1f7e59217a9043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e71b3bc07f12b1e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T02:51:27.643052Z",
     "start_time": "2024-09-15T02:51:27.639944Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "12d17b3c2d404d16",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T15:28:33.536951Z",
     "start_time": "2024-09-15T15:28:33.534768Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6f6fa66123ca65a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:02:36.365948Z",
     "start_time": "2024-09-15T18:02:34.869333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.evaluate_helper_methods import load_sepsis_model\n",
    "from utils.path_utils import project_root\n",
    "import os\n",
    "\n",
    "model_path = os.path.join(project_root(), 'results', 'simmtm', 'saved_models', 'finetune_ep16.pt')\n",
    "model = load_sepsis_model(d_input=336, d_channel=40, d_output=2, model_name=model_path,\n",
    "                          pre_model=\"simmtm\")\n"
   ],
   "id": "d360583e1280b377",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:17:23.162317Z",
     "start_time": "2024-09-15T18:17:18.782160Z"
    }
   },
   "cell_type": "code",
   "source": "torch.load(model_path)['classifier']",
   "id": "79ca166a2a37eab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('logits.weight',\n",
       "              tensor([[-0.0013, -0.0006, -0.0009,  ...,  0.0029, -0.0038, -0.0021],\n",
       "                      [ 0.0014,  0.0022, -0.0012,  ..., -0.0052,  0.0039,  0.0008],\n",
       "                      [ 0.0004, -0.0007,  0.0008,  ..., -0.0010, -0.0008, -0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0002, -0.0008, -0.0016,  ...,  0.0002,  0.0033, -0.0005],\n",
       "                      [-0.0012,  0.0015,  0.0017,  ..., -0.0072,  0.0074,  0.0077],\n",
       "                      [ 0.0015, -0.0015,  0.0009,  ...,  0.0008,  0.0025,  0.0017]],\n",
       "                     device='cuda:0')),\n",
       "             ('logits.bias',\n",
       "              tensor([-2.5756e-03,  2.3701e-03, -8.7043e-04, -2.2944e-03,  8.7966e-04,\n",
       "                      -1.0887e-03, -9.9310e-04, -1.3832e-03, -1.2956e-03, -2.0353e-03,\n",
       "                       5.4989e-05, -4.9039e-04,  8.6841e-04, -7.9175e-04, -3.5971e-03,\n",
       "                       3.8617e-03, -1.0247e-04, -3.8091e-03, -3.1552e-03, -3.6906e-04,\n",
       "                       2.5187e-03,  3.0248e-03,  6.2643e-04,  2.9980e-04,  7.0791e-04,\n",
       "                       1.1497e-03,  8.4284e-04,  1.4906e-04,  7.4519e-03,  6.3431e-04,\n",
       "                       9.7682e-04,  2.2093e-03, -3.6150e-04, -2.3753e-04,  2.1409e-03,\n",
       "                      -3.1568e-03,  9.8013e-04,  1.6852e-03,  8.6167e-04, -4.7611e-03,\n",
       "                       2.5121e-03, -1.7536e-03,  6.6958e-03,  2.4177e-04, -1.7764e-03,\n",
       "                       1.0391e-03, -4.0721e-03,  1.2632e-03,  6.5811e-04,  4.6043e-04,\n",
       "                       2.6405e-03,  1.6953e-03, -7.0296e-04, -4.4314e-03, -8.9477e-04,\n",
       "                      -1.9709e-03,  2.9940e-03,  4.1085e-03,  1.3693e-03, -1.6653e-04,\n",
       "                      -8.5708e-04,  2.6691e-03,  8.6941e-04, -1.7458e-03], device='cuda:0')),\n",
       "             ('logits_simple.weight',\n",
       "              tensor([[-0.1087,  0.1018,  0.1188,  0.1259, -0.0091, -0.0146,  0.0054,  0.0829,\n",
       "                        0.1347,  0.0294,  0.0543, -0.0967, -0.1134, -0.0873, -0.0426, -0.0455,\n",
       "                        0.0423, -0.0455, -0.0359,  0.0229,  0.0591,  0.1419,  0.0212, -0.0326,\n",
       "                       -0.1213,  0.1320,  0.0754,  0.0396,  0.0822, -0.0388, -0.0376, -0.1202,\n",
       "                        0.1111,  0.0581,  0.0033, -0.0740, -0.1119, -0.0646,  0.0448,  0.0579,\n",
       "                       -0.1035,  0.0668,  0.0835, -0.0807, -0.0862,  0.1051,  0.1083,  0.1229,\n",
       "                       -0.0306,  0.0966,  0.0583,  0.0296,  0.0445,  0.0252, -0.1064,  0.0080,\n",
       "                       -0.0095,  0.1094,  0.0239,  0.0688,  0.0972,  0.0198,  0.1100,  0.1124],\n",
       "                      [ 0.0772,  0.0574,  0.0114, -0.0935,  0.0237,  0.0938,  0.1072, -0.0415,\n",
       "                       -0.0745, -0.1112, -0.0801,  0.1042, -0.0507, -0.0190, -0.1383,  0.0778,\n",
       "                       -0.0975, -0.0131, -0.0085, -0.0854,  0.1062,  0.0624,  0.0409, -0.0041,\n",
       "                       -0.0937,  0.1104, -0.0404,  0.1258,  0.1094,  0.0229, -0.0249,  0.1014,\n",
       "                        0.0366,  0.0613, -0.0922,  0.0796,  0.0736, -0.1021,  0.0111,  0.1206,\n",
       "                        0.0991,  0.0419, -0.0248,  0.1024,  0.1264, -0.0166, -0.0920,  0.0021,\n",
       "                       -0.1086,  0.0198, -0.0382, -0.0940, -0.1190,  0.0602, -0.0180, -0.0976,\n",
       "                       -0.0485, -0.0634,  0.1234, -0.0668,  0.0034, -0.1397, -0.1273, -0.0546]],\n",
       "                     device='cuda:0')),\n",
       "             ('logits_simple.bias',\n",
       "              tensor([0.0519, 0.0722], device='cuda:0'))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c4e9e917c22ff42e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:22:52.533147Z",
     "start_time": "2024-09-15T18:22:52.460727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configs = Config()\n",
    "classifier = target_classifier(configs=configs)"
   ],
   "id": "961ffb366fa00050",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:22:57.821371Z",
     "start_time": "2024-09-15T18:22:57.818362Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6e8d8d3503885f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_classifier(\n",
       "  (logits): Linear(in_features=192512, out_features=64, bias=True)\n",
       "  (logits_simple): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T15:59:22.920520Z",
     "start_time": "2024-09-15T15:59:22.918701Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "595708fecee6ba5f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:00:16.200591Z",
     "start_time": "2024-09-15T16:00:15.891235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "test_data = torch.load(os.path.join(project_root(), 'data', 'test_data', 'simmtm', 'test.pt'))['samples']\n"
   ],
   "id": "f48564a2eeed9fcf",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:06:24.652829Z",
     "start_time": "2024-09-15T16:06:24.650902Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "75e91fc0e75291de",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:06:52.411708Z",
     "start_time": "2024-09-15T16:06:52.398669Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "1d47b53b411323e4",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:08:44.721676Z",
     "start_time": "2024-09-15T16:08:44.708916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_setB_all_files = os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training',\n",
    "                         'training_setB')\n",
    "test_setB_files = os.listdir(test_setB_all_files)\n",
    "test_setB_files.sort()\n",
    "test_setB_files.remove('index.html')\n",
    "\n",
    "test_setB_files = [test_setB_files[i] for i in test_indices]\n"
   ],
   "id": "a7c2ccc8e4d4b26",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:16:05.258640Z",
     "start_time": "2024-09-15T16:15:35.191282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_path = os.path.join(project_root(), 'data', 'test_data', 'simmtm', 'psv_files')\n",
    "for pidx in test_setB_files:\n",
    "    pdata = pd.read_csv(os.path.join(test_setB_all_files, pidx), sep='|')\n",
    "    pdata.to_csv(os.path.join(save_path, pidx), sep='|', index=False)\n"
   ],
   "id": "4c401801fd054fc2",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:16:05.261393Z",
     "start_time": "2024-09-15T16:16:05.259803Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d9210aa2d11c0419",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:39:19.383313Z",
     "start_time": "2024-09-15T16:39:15.698619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils.path_utils import project_root\n",
    "from utils.pretrain_utils.data import get_train_val_test_indices\n",
    "\n",
    "test_indices, finetune_indices = get_train_val_test_indices(\n",
    "        sepsis_file='is_sepsis_finetune_B.txt', save_distributions=True,\n",
    "        dset='Bb')\n",
    "\n",
    "test_setB_all_files = os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training',\n",
    "                         'training_setB')\n",
    "test_setB_files = os.listdir(test_setB_all_files)\n",
    "test_setB_files.sort()\n",
    "test_setB_files.remove('index.html')\n",
    "\n",
    "test_setB_files = [test_setB_files[i] for i in test_indices]\n",
    "\n",
    "for pidx in test_setB_files:\n",
    "    break\n",
    "    \n",
    "save_path = os.path.join(project_root(), 'data', 'test_data', 'simmtm', 'psv_files')\n",
    "pd.read_csv(os.path.join(save_path, pidx), sep='|')"
   ],
   "id": "755d73d5d746dd79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       HR  O2Sat  Temp    SBP    MAP    DBP  Resp  EtCO2  BaseExcess  HCO3  \\\n",
       "0     NaN    NaN   NaN    NaN    NaN    NaN   NaN    NaN         NaN   NaN   \n",
       "1    72.0  100.0  36.6  135.0  108.0   94.0  15.0    NaN         NaN   NaN   \n",
       "2   102.0  100.0   NaN  142.0  115.0   86.0  14.0    NaN         NaN   NaN   \n",
       "3    78.0   96.0   NaN  133.0  108.0   95.0  16.0    NaN         NaN   NaN   \n",
       "4    80.0   97.0   NaN  131.0  111.0   95.0  16.0    NaN         NaN   NaN   \n",
       "5    84.0   98.0  36.2  137.0  107.0   94.0  15.0    NaN         NaN   NaN   \n",
       "6    78.0   97.0   NaN  135.0  109.0   94.0  16.0    NaN         NaN   NaN   \n",
       "7    74.0   97.0   NaN  132.0  109.0   87.0  14.0    NaN         NaN   NaN   \n",
       "8    80.0   98.0   NaN  150.0  114.0   90.0  15.0    NaN         NaN   NaN   \n",
       "9    78.0   98.0  36.8  148.0  108.0   93.0  14.0    NaN         NaN   NaN   \n",
       "10   74.5   96.0   NaN  137.5  116.5  102.5   NaN    NaN         NaN   NaN   \n",
       "11   84.0   99.0   NaN  160.0  118.0   87.0  16.0    NaN         NaN   NaN   \n",
       "12    NaN    NaN   NaN    NaN    NaN    NaN   NaN    NaN         NaN   NaN   \n",
       "13  112.0   99.0  35.8  158.0  112.0   93.0  14.0    NaN         NaN   NaN   \n",
       "14   72.0   98.0  36.2  145.0    NaN   88.0  14.0    NaN         NaN   NaN   \n",
       "15   98.0   98.0   NaN  142.0  102.0   78.0  16.0    NaN         NaN   NaN   \n",
       "16    NaN    NaN   NaN    NaN    NaN    NaN   NaN    NaN         NaN   NaN   \n",
       "\n",
       "    ...   WBC  Fibrinogen  Platelets  Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "1   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "2   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "3   ...  13.4         NaN       69.0   23       1    NaN    NaN       -28.85   \n",
       "4   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "5   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "6   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "7   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "8   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "9   ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "10  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "11  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "12  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "13  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "14  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "15  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "16  ...   NaN         NaN        NaN   23       1    NaN    NaN       -28.85   \n",
       "\n",
       "    ICULOS  SepsisLabel  \n",
       "0        1            0  \n",
       "1        2            0  \n",
       "2        3            0  \n",
       "3        4            0  \n",
       "4        5            0  \n",
       "5        6            0  \n",
       "6        7            0  \n",
       "7        8            0  \n",
       "8        9            0  \n",
       "9       10            0  \n",
       "10      11            0  \n",
       "11      12            0  \n",
       "12      13            0  \n",
       "13      14            0  \n",
       "14      15            0  \n",
       "15      16            0  \n",
       "16      17            0  \n",
       "\n",
       "[17 rows x 41 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Fibrinogen</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>135.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.2</td>\n",
       "      <td>137.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>148.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>74.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137.5</td>\n",
       "      <td>116.5</td>\n",
       "      <td>102.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>84.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>112.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>158.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.2</td>\n",
       "      <td>145.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.85</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 41 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:15:31.666688Z",
     "start_time": "2024-09-15T16:15:31.665040Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6fdb9e3b02c4d350",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:48:30.348775Z",
     "start_time": "2024-09-15T16:48:30.346546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# torch.load(os.path.join(project_root(), 'results', 'simmtm', 'saved_models', 'ckp_ep9.pt'))['model_state_dict']"
   ],
   "id": "ac9ec8ab87a8be4c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:49:09.393511Z",
     "start_time": "2024-09-15T16:49:09.254354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.simmtm.config import Config\n",
    "from models.simmtm.model import TFC, target_classifier\n",
    "\n",
    "config = Config()\n",
    "classifier = target_classifier(config).to('cuda')\n"
   ],
   "id": "d9acb3ab57655c84",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T16:49:43.678359Z",
     "start_time": "2024-09-15T16:49:43.676184Z"
    }
   },
   "cell_type": "code",
   "source": "[8, 15 (0.89), 16 (1), 17 (1)]",
   "id": "6d2f46245e123e90",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "134ec7bccf2a316f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
