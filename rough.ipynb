{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T00:34:23.387740Z",
     "start_time": "2024-06-14T00:34:23.385356Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from setup_data import DataSetup\n",
    "\n",
    "from utils.config import gtn_param\n",
    "\n",
    "from train_gtn import GatedTransformerNetwork, load_model, initialize_experiment\n",
    "from utils.loader import make_loader\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f889adff505e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T00:39:14.697846Z",
     "start_time": "2024-06-14T00:39:14.693839Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_file = \"final_dataset.pickle\"\n",
    "# training_examples, lengths_list, is_sepsis, writer, destination_path = initialize_experiment(data_file)\n",
    "# train_loader, test_loader = make_loader(training_examples, lengths_list, is_sepsis, 2048, mode='window')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc1bcddd5e050bc5",
   "metadata": {},
   "source": [
    "- **1. Load all the data atonce without windowing. (Cuz we need to predict before hours of sepsis occured for each patient).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91bbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def platelets_sofa(platelets):\n",
    "    s_score = 0\n",
    "    if platelets > 150:\n",
    "        s_score += 0\n",
    "    elif platelets >= 101 and platelets <= 150:\n",
    "        s_score += 1\n",
    "    elif platelets >= 51 and platelets <= 100:\n",
    "        s_score += 2\n",
    "    elif platelets >= 21 and platelets <= 50:\n",
    "        s_score += 3\n",
    "    elif platelets <= 20:\n",
    "        s_score += 4\n",
    "\n",
    "    return s_score\n",
    "\n",
    "\n",
    "def total_bilirubin_sofa(bilirubin):\n",
    "    s_score = 0\n",
    "    if bilirubin < 1.2:\n",
    "        s_score += 0\n",
    "    elif bilirubin >= 1.2 and bilirubin <= 1.9:\n",
    "        s_score += 1\n",
    "    elif bilirubin >= 2.0 and bilirubin <= 5.9:\n",
    "        s_score += 2\n",
    "    elif bilirubin >= 6 and bilirubin <= 11.9:\n",
    "        s_score += 3\n",
    "    elif bilirubin >= 12.0:\n",
    "        s_score += 4\n",
    "\n",
    "    return s_score\n",
    "\n",
    "\n",
    "def map_sofa(map):\n",
    "    s_score = 0\n",
    "    if map >= 70:\n",
    "        s_score += 0\n",
    "    elif map < 70:\n",
    "        s_score += 1\n",
    "\n",
    "    return s_score\n",
    "\n",
    "\n",
    "def sofa_score(row):\n",
    "    platelets_score = row['Platelets_SOFA']\n",
    "    bilirubin_score = row['Bilirubin_total_SOFA']\n",
    "    map_sofa = row['MAP_SOFA']\n",
    "\n",
    "    return platelets_score + bilirubin_score + map_sofa\n",
    "\n",
    "\n",
    "def detect_sofa_change(data, time_window=24):\n",
    "    data['SOFA_score_diff'] = data['SOFA_score'].diff(periods=time_window)\n",
    "    data['SOFA_deterioration'] = (data['SOFA_score_diff'] >= 2).astype(int)\n",
    "    data['SOFA_score_diff'] = data['SOFA_score_diff'].fillna(value=0)\n",
    "    # data['SOFA_score_diff'].fillna(value=0, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def respiratory_rate_qsofa(respiratory_rate):\n",
    "    q_score = 0\n",
    "    if respiratory_rate >= 22.0:\n",
    "        q_score += 1\n",
    "\n",
    "    return q_score\n",
    "\n",
    "\n",
    "def sbp_qsofa(sbp):\n",
    "    q_score = 0\n",
    "    if sbp < 100.0:\n",
    "        q_score += 1\n",
    "\n",
    "    return q_score\n",
    "\n",
    "\n",
    "def qsofa_score(row):\n",
    "    resp_score = row['ResP_qSOFA']\n",
    "    sbp_score = row['SBP_qSOFA']\n",
    "\n",
    "    return sbp_score + resp_score\n",
    "\n",
    "\n",
    "def q_sofa_indicator(row):\n",
    "    resp = row['ResP_qSOFA']\n",
    "    sbp = row['SBP_qSOFA']\n",
    "    q_score = 0\n",
    "    if resp > 0 and sbp > 0:\n",
    "        q_score += 1\n",
    "    return q_score\n",
    "\n",
    "\n",
    "def sofa_indicator(row):\n",
    "    # 2+ points indicates organ dysfunction\n",
    "    platelets = row['Platelets_SOFA']\n",
    "    bilirubin_total = row['Bilirubin_total_SOFA']\n",
    "    map = row['MAP_SOFA']\n",
    "\n",
    "    total_points = platelets + bilirubin_total + map\n",
    "\n",
    "    q_score = 0\n",
    "    if total_points > 2:\n",
    "        q_score += 1\n",
    "    return q_score\n",
    "\n",
    "\n",
    "def detect_qsofa_change(data, time_window=24):\n",
    "    data['qSOFA_score_diff'] = data['qSOFA_score'].diff(periods=time_window)\n",
    "    data['qSOFA_deterioration'] = (data['qSOFA_score_diff'] >= 2).astype(int)\n",
    "\n",
    "    data['qSOFA_score_diff'] = data['qSOFA_score_diff'].fillna(value=0)\n",
    "    # data['qSOFA_score_diff'].fillna(value=0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def mortality_sofa(row):\n",
    "    # 2+ points indicates organ dysfunction\n",
    "    platelets = row['Platelets_SOFA']\n",
    "    bilirubin_total = row['Bilirubin_total_SOFA']\n",
    "    map = row['MAP_SOFA']\n",
    "\n",
    "    total_points = platelets + bilirubin_total + map\n",
    "\n",
    "    mortality_rate = 0\n",
    "    if total_points > 1 and total_points <= 9:\n",
    "        mortality_rate += 0.30\n",
    "    elif total_points >= 10 and total_points < 14:\n",
    "        mortality_rate += 0.50\n",
    "    elif total_points >= 14:\n",
    "        mortality_rate += 0.95\n",
    "\n",
    "    return mortality_rate\n",
    "\n",
    "\n",
    "def temp_sirs(temp):\n",
    "    sirs_score = 0\n",
    "    if temp < 36 or temp >= 38:\n",
    "        sirs_score += 1\n",
    "\n",
    "    return sirs_score\n",
    "\n",
    "\n",
    "def heart_rate_sirs(heart_rate):\n",
    "    sirs_score = 0\n",
    "    if heart_rate > 90:\n",
    "        sirs_score += 1\n",
    "\n",
    "    return sirs_score\n",
    "\n",
    "\n",
    "def resp_sirs(resp):\n",
    "    sirs_score = 0\n",
    "    if resp > 20:\n",
    "        sirs_score += 1\n",
    "\n",
    "    return sirs_score\n",
    "\n",
    "\n",
    "def paco2_sirs(paco2):\n",
    "    sirs_score = 0\n",
    "    if paco2 < 32:\n",
    "        sirs_score += 1\n",
    "\n",
    "    return sirs_score\n",
    "\n",
    "\n",
    "def wbc_sirs(wbc):\n",
    "    sirs_score = 0\n",
    "    if wbc * 1000 < 4000 or wbc * 1000 > 12000:\n",
    "        sirs_score += 1\n",
    "    return sirs_score\n",
    "\n",
    "\n",
    "def t_suspicion(patient_data):\n",
    "    \"\"\"\n",
    "    Since we don't have information about IV antibiotics and blood cultures,\n",
    "    we are is considering that patient have infection if any 2 SIRS criteria are met\n",
    "    \"\"\"\n",
    "    patient_data['infection_proxy'] = (patient_data[['Temp_sirs', 'HR_sirs', 'Resp_sirs']].eq(1).sum(axis=1) >= 2).astype(int)\n",
    "\n",
    "    # t_suspicion is the first hour of (ICULOS) where infection proxy is positive at time t\n",
    "    patient_data['t_suspicion'] = patient_data.groupby(['PatientID'])['ICULOS'].transform(\n",
    "        lambda x: x[patient_data['infection_proxy'] == 1].min() if (patient_data['infection_proxy'] == 1).any() else 0)\n",
    "\n",
    "    return patient_data\n",
    "\n",
    "\n",
    "def t_sofa(data):\n",
    "    \"\"\"\n",
    "    Two-point deterioration in SOFA score at time t but within a 24-hour period.\n",
    "    \"\"\"\n",
    "    data['t_sofa'] = data['SOFA_score_diff'].where((abs(data['SOFA_score_diff']) >= 2) & (data['ICULOS'] <= 24),\n",
    "                                                   other=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def t_sepsis(row):\n",
    "    if pd.isna(row['t_suspicion']) or row['t_suspicion'] == 0 or row['t_sofa'] == 0:\n",
    "        return 0\n",
    "    if row['t_suspicion'] - 24 <= row['t_sofa'] <= row['t_suspicion'] + 12:\n",
    "        return min(row['t_suspicion'], row['t_sofa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b32d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sepsis_model():\n",
    "    config = gtn_param\n",
    "    d_input, d_channel, d_output = 336, 63, 2  # (time_steps (window_size), channels, num_classes)\n",
    "    model = GatedTransformerNetwork(d_model=config['d_model'], d_input=d_input, d_channel=d_channel,\n",
    "                                    d_output=d_output, d_hidden=config['d_hidden'], q=config['q'],\n",
    "                                    v=config['v'], h=config['h'], N=config['N'], dropout=config['dropout'],\n",
    "                                    pe=config['pe'], mask=config['mask'], device='cuda').to('cuda')\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_challenge_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        header = f.readline().strip()\n",
    "        column_names = header.split('|')\n",
    "        data = np.loadtxt(f, delimiter='|')\n",
    "\n",
    "    # Ignore SepsisLabel column if present.\n",
    "    if column_names[-1] == 'SepsisLabel':\n",
    "        column_names = column_names[:-1]\n",
    "        data = data[:, :-1]\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_sepsis_score(data, model):\n",
    "\n",
    "    columns = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp',\n",
    "       'EtCO2', 'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST',\n",
    "       'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "       'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
    "       'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
    "       'Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2',\n",
    "       'HospAdmTime', 'ICULOS']\n",
    "\n",
    "    # Reformatting data into DataFrame to add features\n",
    "    data_df = pd.DataFrame(data, columns=columns)\n",
    "    patient_data = data_df.fillna(0)\n",
    "    \n",
    "\n",
    "    patient_data['MAP_SOFA'] = patient_data['MAP'].apply(map_sofa)\n",
    "    patient_data['Bilirubin_total_SOFA'] = patient_data['Bilirubin_total'].apply(total_bilirubin_sofa)\n",
    "    patient_data['Platelets_SOFA'] = patient_data['Platelets'].apply(platelets_sofa)\n",
    "    patient_data['SOFA_score'] = patient_data.apply(sofa_score, axis=1)\n",
    "    patient_data = detect_sofa_change(patient_data)\n",
    "\n",
    "    patient_data['ResP_qSOFA'] = patient_data['Resp'].apply(respiratory_rate_qsofa)\n",
    "    patient_data['SBP_qSOFA'] = patient_data['SBP'].apply(sbp_qsofa)\n",
    "    patient_data['qSOFA_score'] = patient_data.apply(qsofa_score, axis=1)\n",
    "    patient_data = detect_qsofa_change(patient_data)\n",
    "\n",
    "    patient_data['qSOFA_indicator'] = patient_data.apply(q_sofa_indicator, axis=1)  # Sepsis detected\n",
    "    patient_data['SOFA_indicator'] = patient_data.apply(sofa_indicator, axis=1)  # Organ Dysfunction occurred\n",
    "    patient_data['Mortality_sofa'] = patient_data.apply(mortality_sofa, axis=1)  # Morality rate\n",
    "\n",
    "    patient_data['Temp_sirs'] = patient_data['Temp'].apply(temp_sirs)\n",
    "    patient_data['HR_sirs'] = patient_data['HR'].apply(heart_rate_sirs)\n",
    "    patient_data['Resp_sirs'] = patient_data['Resp'].apply(resp_sirs)\n",
    "    patient_data['paco2_sirs'] = patient_data['PaCO2'].apply(resp_sirs)\n",
    "    patient_data['wbc_sirs'] = patient_data['WBC'].apply(wbc_sirs)\n",
    "\n",
    "    patient_data = t_suspicion(patient_data)\n",
    "    patient_data = t_sofa(patient_data)\n",
    "    patient_data['t_sepsis'] = patient_data.apply(t_sepsis, axis=1)\n",
    "\n",
    "    patient_data = torch.tensor(patient_data.values).unsqueeze(0)\n",
    "    print(patient_data.shape)\n",
    "\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "    predictions = []\n",
    "    probas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patient_data = patient_data.to('cuda').float()\n",
    "        outputs, _, _, _, _, _, _ = model(patient_data, stage='test')\n",
    "    \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        probas.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    return patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da107c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sepsis model...\n",
      "Predicting sepsis labels...\n",
      "    1/20336...\n",
      "torch.Size([1, 1, 63])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (336) at non-singleton dimension 0.  Target sizes: [1, 256].  Tensor sizes: [336, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m model, data, current_data, data_df\n\u001b[0;32m---> 48\u001b[0m model, data, current_data, data_df \u001b[39m=\u001b[39m evaluate()\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_rows):\n\u001b[1;32m     36\u001b[0m     current_data \u001b[39m=\u001b[39m data[:t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m     data_df \u001b[39m=\u001b[39m get_sepsis_score(current_data, model)\n\u001b[1;32m     38\u001b[0m     \u001b[39m# current_score, current_label = get_sepsis_score(current_data, model)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[39m# scores[t] = current_score\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# labels[t] = current_labels\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mget_sepsis_score\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     73\u001b[0m     patient_data \u001b[39m=\u001b[39m patient_data\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 74\u001b[0m     outputs, _, _, _, _, _, _ \u001b[39m=\u001b[39m model(patient_data, stage\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m     probabilities \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SepsisEarlyPrediction/models/gtn.py:121\u001b[0m, in \u001b[0;36mGatedTransformerNetwork.forward\u001b[0;34m(self, x, stage)\u001b[0m\n\u001b[1;32m    119\u001b[0m temp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(temp)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    120\u001b[0m temp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(position\u001b[39m.\u001b[39mfloat(), temp)  \u001b[39m# shape:[input, d_model/2]\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m pe[:, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(temp)\n\u001b[1;32m    122\u001b[0m pe[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(temp)\n\u001b[1;32m    124\u001b[0m encoding_1 \u001b[39m=\u001b[39m encoding_1 \u001b[39m+\u001b[39m pe\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (336) at non-singleton dimension 0.  Target sizes: [1, 256].  Tensor sizes: [336, 256]"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    input_directory = \"/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setA/\"\n",
    "    # input_directory = \"/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setB/\"\n",
    "    output_directory = \"./data/test_output/\"\n",
    "\n",
    "    # Find files.\n",
    "    files = []\n",
    "    for f in os.listdir(input_directory):\n",
    "        if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "            files.append(f)\n",
    "    \n",
    "    # files.sort()\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    \n",
    "    # Load model.\n",
    "    print('Loading sepsis model...')\n",
    "    model = load_sepsis_model()\n",
    "\n",
    "    # Iterate over files.\n",
    "    print('Predicting sepsis labels...')\n",
    "    num_files = len(files)\n",
    "    for i, f in enumerate(files):\n",
    "        print('    {}/{}...'.format(i+1, num_files))\n",
    "\n",
    "        # Load data.\n",
    "        input_file = os.path.join(input_directory, f)\n",
    "        data = load_challenge_data(input_file)\n",
    "\n",
    "        # Make predictions.\n",
    "        num_rows = len(data)  # Number of patient recordings\n",
    "        scores = np.zeros(num_rows)\n",
    "        labels = np.zeros(num_rows)\n",
    "        \n",
    "        for t in range(num_rows):\n",
    "            current_data = data[:t+1]\n",
    "            data_df = get_sepsis_score(current_data, model)\n",
    "            # current_score, current_label = get_sepsis_score(current_data, model)\n",
    "            # scores[t] = current_score\n",
    "            # labels[t] = current_labels\n",
    "\n",
    "            break\n",
    "        \n",
    "        break\n",
    "    \n",
    "    return model, data, current_data, data_df\n",
    "\n",
    "model, data, current_data, data_df = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c958190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2b6868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datafile used: final_dataset.pickle\n",
      "Total number of patients: 40336\n",
      "Min recordings: 8 & Max recordings: 336\n",
      "Distribution of the SepsisLabel: \n",
      "0    37404\n",
      "1     2932\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding...: 100%|██████████| 32268/32268 [00:25<00:00, 1257.11it/s]\n",
      "Padding...: 100%|██████████| 8067/8067 [00:05<00:00, 1378.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data_file = \"final_dataset.pickle\"\n",
    "training_examples, lengths_list, is_sepsis, writer, destination_path = initialize_experiment(data_file)\n",
    "train_loader, test_loader = make_loader(training_examples, lengths_list, is_sepsis, 128, mode='padding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4aedba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 222, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n",
      "File \u001b[0;32m/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 222, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/localscratch/neeresh/envs/timeseries/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a6b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 41)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Fibrinogen</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unit1</th>\n",
       "      <th>Unit2</th>\n",
       "      <th>HospAdmTime</th>\n",
       "      <th>ICULOS</th>\n",
       "      <th>SepsisLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>75.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.0</td>\n",
       "      <td>86.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.0</td>\n",
       "      <td>91.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HR  O2Sat  Temp    SBP    MAP  DBP  Resp  EtCO2  BaseExcess  HCO3  ...  \\\n",
       "0    NaN    NaN   NaN    NaN    NaN  NaN   NaN    NaN         NaN   NaN  ...   \n",
       "1   97.0   95.0   NaN   98.0  75.33  NaN  19.0    NaN         NaN   NaN  ...   \n",
       "2   89.0   99.0   NaN  122.0  86.00  NaN  22.0    NaN         NaN   NaN  ...   \n",
       "3   90.0   95.0   NaN    NaN    NaN  NaN  30.0    NaN        24.0   NaN  ...   \n",
       "4  103.0   88.5   NaN  122.0  91.33  NaN  24.5    NaN         NaN   NaN  ...   \n",
       "\n",
       "   WBC  Fibrinogen  Platelets    Age  Gender  Unit1  Unit2  HospAdmTime  \\\n",
       "0  NaN         NaN        NaN  83.14       0    NaN    NaN        -0.03   \n",
       "1  NaN         NaN        NaN  83.14       0    NaN    NaN        -0.03   \n",
       "2  NaN         NaN        NaN  83.14       0    NaN    NaN        -0.03   \n",
       "3  NaN         NaN        NaN  83.14       0    NaN    NaN        -0.03   \n",
       "4  NaN         NaN        NaN  83.14       0    NaN    NaN        -0.03   \n",
       "\n",
       "   ICULOS  SepsisLabel  \n",
       "0       1            0  \n",
       "1       2            0  \n",
       "2       3            0  \n",
       "3       4            0  \n",
       "4       5            0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setA/p000001.psv', sep='|')\n",
    "print(sample.shape)\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54, 353)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = (366 - len(sample))\n",
    "print(pad)\n",
    "np.pad(sample.values, ((0, 0), (pad, 0)), mode='constant').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344a3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8845dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b91e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413e8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58129c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sepsis_score(loader, model):\n",
    "#     model.eval()\n",
    "#     model.to('cuda')\n",
    "#     predictions = []\n",
    "#     probas = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in tqdm.tqdm(loader, desc=\"Generating Predictions\", total=len(loader)):\n",
    "#             inputs = inputs.to('cuda').float()\n",
    "#             targets = targets.to('cuda')\n",
    "#             outputs, _, _, _, _, _, _ = model(inputs, stage='test')\n",
    "        \n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "#             predictions.extend(predicted.cpu().numpy())\n",
    "#             probas.extend(probabilities.cpu().numpy())\n",
    "\n",
    "#             break\n",
    "\n",
    "#     return predictions, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3719601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_sepsis_model()\n",
    "# labels, scores = get_sepsis_score(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ff1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae4186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999cd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
