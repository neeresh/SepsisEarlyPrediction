{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:39:08.067214Z",
     "start_time": "2024-06-15T15:39:06.769662Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.evaluate_helper_methods import *\n",
    "from utils.path_utils import project_root\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf41bf98",
   "metadata": {},
   "source": [
    "- **Labels Directory: all patients true labels in each file**<br>\n",
    "- **Predictions Directory: all patients \"score, predictions\" for each time step in the format (PredictedProbability|PredictedLabel)**<br>\n",
    "- **These can be created using driver.py file.**<br>\n",
    "- **First finish it!!!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f930347",
   "metadata": {},
   "source": [
    "- **When applying windowing method, make sure the following:**<br>\n",
    "- **1. After model is trained on windowing technique, model accepts: (6, 63, 2) -> (window_size, features, num_classes)**\n",
    "- **2. Since the predictions are based on t, t+1, t+2, ... t+n, fix the shape of the sample for <= t+6 (padding or what ever)**\n",
    "- **3. But if the size of the window is larger, we need to pad for longer iterations and thus we cannot get good score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b108dee749030855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:39:34.852058Z",
     "start_time": "2024-06-15T15:39:34.745492Z"
    }
   },
   "outputs": [],
   "source": [
    "d_input, d_channel, d_output = 336, 63, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ed234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edfeb3ab43f433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:41.157680Z",
     "start_time": "2024-06-15T07:33:41.148334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sepsis_score(data, model):\n",
    "\n",
    "    columns = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp',\n",
    "       'EtCO2', 'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST',\n",
    "       'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "       'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
    "       'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
    "       'Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2',\n",
    "       'HospAdmTime', 'ICULOS']\n",
    "\n",
    "    # Reformatting data into DataFrame to add features\n",
    "    patient_data = pd.DataFrame(data, columns=columns)\n",
    "    patient_data = patient_data.fillna(0)\n",
    "    \n",
    "    patient_data['MAP_SOFA'] = patient_data['MAP'].apply(map_sofa)\n",
    "    patient_data['Bilirubin_total_SOFA'] = patient_data['Bilirubin_total'].apply(total_bilirubin_sofa)\n",
    "    patient_data['Platelets_SOFA'] = patient_data['Platelets'].apply(platelets_sofa)\n",
    "    patient_data['SOFA_score'] = patient_data.apply(sofa_score, axis=1)\n",
    "    patient_data = detect_sofa_change(patient_data)\n",
    "\n",
    "    patient_data['ResP_qSOFA'] = patient_data['Resp'].apply(respiratory_rate_qsofa)\n",
    "    patient_data['SBP_qSOFA'] = patient_data['SBP'].apply(sbp_qsofa)\n",
    "    patient_data['qSOFA_score'] = patient_data.apply(qsofa_score, axis=1)\n",
    "    patient_data = detect_qsofa_change(patient_data)\n",
    "\n",
    "    patient_data['qSOFA_indicator'] = patient_data.apply(q_sofa_indicator, axis=1)  # Sepsis detected\n",
    "    patient_data['SOFA_indicator'] = patient_data.apply(sofa_indicator, axis=1)  # Organ Dysfunction occurred\n",
    "    patient_data['Mortality_sofa'] = patient_data.apply(mortality_sofa, axis=1)  # Morality rate\n",
    "\n",
    "    patient_data['Temp_sirs'] = patient_data['Temp'].apply(temp_sirs)\n",
    "    patient_data['HR_sirs'] = patient_data['HR'].apply(heart_rate_sirs)\n",
    "    patient_data['Resp_sirs'] = patient_data['Resp'].apply(resp_sirs)\n",
    "    patient_data['paco2_sirs'] = patient_data['PaCO2'].apply(resp_sirs)\n",
    "    patient_data['wbc_sirs'] = patient_data['WBC'].apply(wbc_sirs)\n",
    "\n",
    "    patient_data = t_suspicion(patient_data)\n",
    "    patient_data = t_sofa(patient_data)\n",
    "    patient_data['t_sepsis'] = patient_data.apply(t_sepsis, axis=1)\n",
    "    \n",
    "    # Padding remaning rows to meet the model requirements\n",
    "    # Each patient file will be (336, 63) -> (Timestamps, features)\n",
    "\n",
    "    # 336 rows are padded dynamically based on how each timestamp for each patient\n",
    "    max_rows = 336\n",
    "    num_features = patient_data.shape[1]\n",
    "    if len(patient_data) < max_rows:\n",
    "        padding = np.zeros((max_rows - len(patient_data), num_features))\n",
    "        patient_data = np.vstack((patient_data, padding))\n",
    "    elif len(patient_data) > max_rows:\n",
    "        patient_data = patient_data.iloc[:max_rows]\n",
    "\n",
    "    patient_data = torch.tensor(patient_data).unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    probas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patient_data = patient_data.to(torch.float32).to(device)\n",
    "        outputs, _, _, _, _, _, _ = model(patient_data, stage='test')\n",
    "    \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        predicted_class = predicted.detach().cpu().numpy()[0]\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "        probas.append(probabilities.detach().cpu().numpy()[0][predicted_class])\n",
    "\n",
    "    return predictions, probas, patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e2e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5b86445cebc3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:44:09.546411Z",
     "start_time": "2024-06-15T07:44:09.295166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GTN model...\n",
      "Model is set to eval() mode...\n",
      "Model is on the deivce: cuda\n",
      "Predicting sepsis labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remaining Files:   0%|          | 35/20336 [00:35<5:39:49,  1.00s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 49\u001B[0m\n\u001B[1;32m     45\u001B[0m         save_challenge_predictions(output_file, scores, labels)\n\u001B[1;32m     47\u001B[0m     \u001B[39mreturn\u001B[39;00m model, data, current_data, data_df\n\u001B[0;32m---> 49\u001B[0m model, data, current_data, data_df \u001B[39m=\u001B[39m evaluate()\n",
      "Cell \u001B[0;32mIn[4], line 40\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m()\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[39mfor\u001B[39;00m t \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(num_rows):\n\u001B[1;32m     39\u001B[0m     current_data \u001B[39m=\u001B[39m data[:t\u001B[39m+\u001B[39m\u001B[39m1\u001B[39m]\n\u001B[0;32m---> 40\u001B[0m     current_labels, current_score, data_df \u001B[39m=\u001B[39m get_sepsis_score(current_data, model)\n\u001B[1;32m     41\u001B[0m     scores[t] \u001B[39m=\u001B[39m current_score[\u001B[39m0\u001B[39m]\n\u001B[1;32m     42\u001B[0m     labels[t] \u001B[39m=\u001B[39m current_labels[\u001B[39m0\u001B[39m]\n",
      "Cell \u001B[0;32mIn[3], line 38\u001B[0m, in \u001B[0;36mget_sepsis_score\u001B[0;34m(data, model)\u001B[0m\n\u001B[1;32m     36\u001B[0m patient_data \u001B[39m=\u001B[39m t_suspicion(patient_data)\n\u001B[1;32m     37\u001B[0m patient_data \u001B[39m=\u001B[39m t_sofa(patient_data)\n\u001B[0;32m---> 38\u001B[0m patient_data[\u001B[39m'\u001B[39m\u001B[39mt_sepsis\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m patient_data\u001B[39m.\u001B[39;49mapply(t_sepsis, axis\u001B[39m=\u001B[39;49m\u001B[39m1\u001B[39;49m)\n\u001B[1;32m     40\u001B[0m \u001B[39m# Padding remaning rows to meet the model requirements\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[39m# Each patient file will be (336, 63) -> (Timestamps, features)\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \n\u001B[1;32m     43\u001B[0m \u001B[39m# 336 rows are padded dynamically based on how each timestamp for each patient\u001B[39;00m\n\u001B[1;32m     44\u001B[0m max_rows \u001B[39m=\u001B[39m \u001B[39m336\u001B[39m\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/frame.py:9423\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[1;32m   9412\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mpandas\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mapply\u001B[39;00m \u001B[39mimport\u001B[39;00m frame_apply\n\u001B[1;32m   9414\u001B[0m op \u001B[39m=\u001B[39m frame_apply(\n\u001B[1;32m   9415\u001B[0m     \u001B[39mself\u001B[39m,\n\u001B[1;32m   9416\u001B[0m     func\u001B[39m=\u001B[39mfunc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   9421\u001B[0m     kwargs\u001B[39m=\u001B[39mkwargs,\n\u001B[1;32m   9422\u001B[0m )\n\u001B[0;32m-> 9423\u001B[0m \u001B[39mreturn\u001B[39;00m op\u001B[39m.\u001B[39;49mapply()\u001B[39m.\u001B[39m__finalize__(\u001B[39mself\u001B[39m, method\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mapply\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/apply.py:678\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mraw:\n\u001B[1;32m    676\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mapply_raw()\n\u001B[0;32m--> 678\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mapply_standard()\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/apply.py:801\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    798\u001B[0m results, res_index \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mapply_series_generator()\n\u001B[1;32m    800\u001B[0m \u001B[39m# wrap results\u001B[39;00m\n\u001B[0;32m--> 801\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mwrap_results(results, res_index)\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/apply.py:838\u001B[0m, in \u001B[0;36mFrameApply.wrap_results\u001B[0;34m(self, results, res_index)\u001B[0m\n\u001B[1;32m    836\u001B[0m     result \u001B[39m=\u001B[39m constructor_sliced(results, dtype\u001B[39m=\u001B[39mnp\u001B[39m.\u001B[39mfloat64)\n\u001B[1;32m    837\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m--> 838\u001B[0m     result \u001B[39m=\u001B[39m constructor_sliced(results)\n\u001B[1;32m    839\u001B[0m result\u001B[39m.\u001B[39mindex \u001B[39m=\u001B[39m res_index\n\u001B[1;32m    841\u001B[0m \u001B[39mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/series.py:472\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    470\u001B[0m         data \u001B[39m=\u001B[39m data\u001B[39m.\u001B[39m_mgr\n\u001B[1;32m    471\u001B[0m \u001B[39melif\u001B[39;00m is_dict_like(data):\n\u001B[0;32m--> 472\u001B[0m     data, index \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_init_dict(data, index, dtype)\n\u001B[1;32m    473\u001B[0m     dtype \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[1;32m    474\u001B[0m     copy \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/series.py:565\u001B[0m, in \u001B[0;36mSeries._init_dict\u001B[0;34m(self, data, index, dtype)\u001B[0m\n\u001B[1;32m    562\u001B[0m     keys, values \u001B[39m=\u001B[39m default_index(\u001B[39m0\u001B[39m), []\n\u001B[1;32m    564\u001B[0m \u001B[39m# Input is now list-like, so rely on \"standard\" construction:\u001B[39;00m\n\u001B[0;32m--> 565\u001B[0m s \u001B[39m=\u001B[39m Series(values, index\u001B[39m=\u001B[39;49mkeys, dtype\u001B[39m=\u001B[39;49mdtype)\n\u001B[1;32m    567\u001B[0m \u001B[39m# Now we just make sure the order is respected, if any\u001B[39;00m\n\u001B[1;32m    568\u001B[0m \u001B[39mif\u001B[39;00m data \u001B[39mand\u001B[39;00m index \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/series.py:425\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    422\u001B[0m name \u001B[39m=\u001B[39m ibase\u001B[39m.\u001B[39mmaybe_extract_name(name, data, \u001B[39mtype\u001B[39m(\u001B[39mself\u001B[39m))\n\u001B[1;32m    424\u001B[0m \u001B[39mif\u001B[39;00m index \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m--> 425\u001B[0m     index \u001B[39m=\u001B[39m ensure_index(index)\n\u001B[1;32m    427\u001B[0m \u001B[39mif\u001B[39;00m dtype \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    428\u001B[0m     dtype \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_validate_dtype(dtype)\n",
      "File \u001B[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/indexes/base.py:7103\u001B[0m, in \u001B[0;36mensure_index\u001B[0;34m(index_like, copy)\u001B[0m\n\u001B[1;32m   7072\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mensure_index\u001B[39m(index_like: Axes, copy: \u001B[39mbool\u001B[39m \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Index:\n\u001B[1;32m   7073\u001B[0m \u001B[39m    \u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m   7074\u001B[0m \u001B[39m    Ensure that we have an index from some index-like object.\u001B[39;00m\n\u001B[1;32m   7075\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   7101\u001B[0m \u001B[39m           )\u001B[39;00m\n\u001B[1;32m   7102\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 7103\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39;49m(index_like, Index):\n\u001B[1;32m   7104\u001B[0m         \u001B[39mif\u001B[39;00m copy:\n\u001B[1;32m   7105\u001B[0m             index_like \u001B[39m=\u001B[39m index_like\u001B[39m.\u001B[39mcopy()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "\n",
    "    # Gathering Files\n",
    "    # input_directory = os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training','training_setA')\n",
    "    \n",
    "    input_directory = \"/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setA/\"\n",
    "    # input_directory = \"/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setB/\"\n",
    "    output_directory = \"./predictions\"\n",
    "\n",
    "    # Find files.\n",
    "    files = []\n",
    "    for f in os.listdir(input_directory):\n",
    "        if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "            files.append(f)\n",
    "    \n",
    "    # files.sort()\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    \n",
    "    # Load Sepsis Model\n",
    "    model = load_sepsis_model(d_input=d_input, d_channel=d_channel, d_output=d_output)\n",
    "\n",
    "    # Iterate over files.\n",
    "    print('Predicting sepsis labels...')\n",
    "    num_files = len(files)\n",
    "    for i, f in tqdm.tqdm(enumerate(files), desc=\"Remaining Files: \", total=num_files):\n",
    "        # print('    {}/{}...'.format(i+1, num_files))\n",
    "\n",
    "        # Load data.\n",
    "        input_file = os.path.join(input_directory, f)\n",
    "        data = load_challenge_data(input_file)\n",
    "\n",
    "        # Make predictions.\n",
    "        num_rows = len(data)  # Number of patient recordings\n",
    "        scores = np.zeros(num_rows)\n",
    "        labels = np.zeros(num_rows)\n",
    "        \n",
    "        for t in range(num_rows):\n",
    "            current_data = data[:t+1]\n",
    "            current_labels, current_score, data_df = get_sepsis_score(current_data, model)\n",
    "            scores[t] = current_score[0]\n",
    "            labels[t] = current_labels[0]\n",
    "        \n",
    "        output_file = os.path.join(output_directory, f)\n",
    "        save_challenge_predictions(output_file, scores, labels)\n",
    "    \n",
    "    return model, data, current_data, data_df\n",
    "\n",
    "model, data, current_data, data_df = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d948b978ff3e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:42.921579Z",
     "start_time": "2024-06-15T07:33:42.920147Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc2c458c15b85e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:22.330952Z",
     "start_time": "2024-06-15T07:33:22.329663Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.evaluate_sepsis_score import evaluate_sepsis_score\n",
    "\n",
    "# Numbers of label and prediction files must be the same\n",
    "evaluate_sepsis_score(label_directory='./labels/', prediction_directory='./predictions/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f1d95a6525fdae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:22.335205Z",
     "start_time": "2024-06-15T07:33:22.333655Z"
    }
   },
   "source": [
    "- **1. Load data from training_setA/training_setB only.**\n",
    "- **2. Divide it into train and test.**\n",
    "- **3. Train the model and evaluate it on test.**\n",
    "- **4. Use the evaluate method to get the predictions.**\n",
    "- **5. Run the get_true_labels for training_setA files.**\n",
    "- **6. run evaluate_sepsis_score.py use: from utils.evaluate_sepsis_score import evaluate_sepsis_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1870eeb818a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:22.337202Z",
     "start_time": "2024-06-15T07:33:22.335851Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034584d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
