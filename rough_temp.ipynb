{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:39:08.067214Z",
     "start_time": "2024-06-15T15:39:06.769662Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.evaluate_helper_methods import *\n",
    "from utils.path_utils import project_root\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf41bf98",
   "metadata": {},
   "source": [
    "- **Labels Directory: all patients true labels in each file**<br>\n",
    "- **Predictions Directory: all patients \"score, predictions\" for each time step in the format (PredictedProbability|PredictedLabel)**<br>\n",
    "- **These can be created using driver.py file.**<br>\n",
    "- **First finish it!!!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f930347",
   "metadata": {},
   "source": [
    "- **When applying windowing method, make sure the following:**<br>\n",
    "- **1. After model is trained on windowing technique, model accepts: (6, 63, 2) -> (window_size, features, num_classes)**\n",
    "- **2. Since the predictions are based on t, t+1, t+2, ... t+n, fix the shape of the sample for <= t+6 (padding or what ever)**\n",
    "- **3. But if the size of the window is larger, we need to pad for longer iterations and thus we cannot get good score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ed234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edfeb3ab43f433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:41.157680Z",
     "start_time": "2024-06-15T07:33:41.148334Z"
    }
   },
   "outputs": [],
   "source": [
    "d_input, d_channel, d_output = 336, 63, 2\n",
    "\n",
    "def get_sepsis_score(data, model):\n",
    "\n",
    "    columns = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp',\n",
    "       'EtCO2', 'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST',\n",
    "       'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
    "       'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
    "       'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
    "       'Fibrinogen', 'Platelets', 'Age', 'Gender', 'Unit1', 'Unit2',\n",
    "       'HospAdmTime', 'ICULOS']\n",
    "\n",
    "    # Reformatting data into DataFrame to add features\n",
    "    patient_data = pd.DataFrame(data, columns=columns)\n",
    "    patient_data = patient_data.fillna(0)\n",
    "    \n",
    "    patient_data['MAP_SOFA'] = patient_data['MAP'].apply(map_sofa)\n",
    "    patient_data['Bilirubin_total_SOFA'] = patient_data['Bilirubin_total'].apply(total_bilirubin_sofa)\n",
    "    patient_data['Platelets_SOFA'] = patient_data['Platelets'].apply(platelets_sofa)\n",
    "    patient_data['SOFA_score'] = patient_data.apply(sofa_score, axis=1)\n",
    "    patient_data = detect_sofa_change(patient_data)\n",
    "\n",
    "    patient_data['ResP_qSOFA'] = patient_data['Resp'].apply(respiratory_rate_qsofa)\n",
    "    patient_data['SBP_qSOFA'] = patient_data['SBP'].apply(sbp_qsofa)\n",
    "    patient_data['qSOFA_score'] = patient_data.apply(qsofa_score, axis=1)\n",
    "    patient_data = detect_qsofa_change(patient_data)\n",
    "\n",
    "    patient_data['qSOFA_indicator'] = patient_data.apply(q_sofa_indicator, axis=1)  # Sepsis detected\n",
    "    patient_data['SOFA_indicator'] = patient_data.apply(sofa_indicator, axis=1)  # Organ Dysfunction occurred\n",
    "    patient_data['Mortality_sofa'] = patient_data.apply(mortality_sofa, axis=1)  # Morality rate\n",
    "\n",
    "    patient_data['Temp_sirs'] = patient_data['Temp'].apply(temp_sirs)\n",
    "    patient_data['HR_sirs'] = patient_data['HR'].apply(heart_rate_sirs)\n",
    "    patient_data['Resp_sirs'] = patient_data['Resp'].apply(resp_sirs)\n",
    "    patient_data['paco2_sirs'] = patient_data['PaCO2'].apply(resp_sirs)\n",
    "    patient_data['wbc_sirs'] = patient_data['WBC'].apply(wbc_sirs)\n",
    "\n",
    "    patient_data = t_suspicion(patient_data)\n",
    "    patient_data = t_sofa(patient_data)\n",
    "    patient_data['t_sepsis'] = patient_data.apply(t_sepsis, axis=1)\n",
    "    \n",
    "    # Padding remaning rows to meet the model requirements\n",
    "    # Each patient file will be (336, 63) -> (Timestamps, features)\n",
    "\n",
    "    # 336 rows are padded dynamically based on how each timestamp for each patient\n",
    "    max_rows = 336\n",
    "    num_features = patient_data.shape[1]\n",
    "    if len(patient_data) < max_rows:\n",
    "        padding = np.zeros((max_rows - len(patient_data), num_features))\n",
    "        patient_data = np.vstack((patient_data, padding))\n",
    "    elif len(patient_data) > max_rows:\n",
    "        patient_data = patient_data.iloc[:max_rows]\n",
    "\n",
    "    patient_data = torch.tensor(patient_data).unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    probas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patient_data = patient_data.to(torch.float32).to(device)\n",
    "        outputs, _, _, _, _, _, _ = model(patient_data, stage='test')\n",
    "    \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        predicted_class = predicted.detach().cpu().numpy()[0]\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "        probas.append(probabilities.detach().cpu().numpy()[0][predicted_class])\n",
    "\n",
    "    return predictions, probas, patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e2e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5b86445cebc3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:44:09.546411Z",
     "start_time": "2024-06-15T07:44:09.295166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GTN model...\n",
      "Model is set to eval() mode...\n",
      "Model is on the deivce: cuda\n",
      "Predicting sepsis labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remaining Files:   0%|          | 35/20336 [00:35<5:39:49,  1.00s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m         save_challenge_predictions(output_file, scores, labels)\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m model, data, current_data, data_df\n\u001b[0;32m---> 49\u001b[0m model, data, current_data, data_df \u001b[39m=\u001b[39m evaluate()\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_rows):\n\u001b[1;32m     39\u001b[0m     current_data \u001b[39m=\u001b[39m data[:t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m     current_labels, current_score, data_df \u001b[39m=\u001b[39m get_sepsis_score(current_data, model)\n\u001b[1;32m     41\u001b[0m     scores[t] \u001b[39m=\u001b[39m current_score[\u001b[39m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m     labels[t] \u001b[39m=\u001b[39m current_labels[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mget_sepsis_score\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m patient_data \u001b[39m=\u001b[39m t_suspicion(patient_data)\n\u001b[1;32m     37\u001b[0m patient_data \u001b[39m=\u001b[39m t_sofa(patient_data)\n\u001b[0;32m---> 38\u001b[0m patient_data[\u001b[39m'\u001b[39m\u001b[39mt_sepsis\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m patient_data\u001b[39m.\u001b[39;49mapply(t_sepsis, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Padding remaning rows to meet the model requirements\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Each patient file will be (336, 63) -> (Timestamps, features)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[39m# 336 rows are padded dynamically based on how each timestamp for each patient\u001b[39;00m\n\u001b[1;32m     44\u001b[0m max_rows \u001b[39m=\u001b[39m \u001b[39m336\u001b[39m\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/apply.py:801\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_series_generator()\n\u001b[1;32m    800\u001b[0m \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/apply.py:838\u001b[0m, in \u001b[0;36mFrameApply.wrap_results\u001b[0;34m(self, results, res_index)\u001b[0m\n\u001b[1;32m    836\u001b[0m     result \u001b[39m=\u001b[39m constructor_sliced(results, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    837\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     result \u001b[39m=\u001b[39m constructor_sliced(results)\n\u001b[1;32m    839\u001b[0m result\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m res_index\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/series.py:472\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    470\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39m_mgr\n\u001b[1;32m    471\u001b[0m \u001b[39melif\u001b[39;00m is_dict_like(data):\n\u001b[0;32m--> 472\u001b[0m     data, index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_dict(data, index, dtype)\n\u001b[1;32m    473\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/series.py:565\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    562\u001b[0m     keys, values \u001b[39m=\u001b[39m default_index(\u001b[39m0\u001b[39m), []\n\u001b[1;32m    564\u001b[0m \u001b[39m# Input is now list-like, so rely on \"standard\" construction:\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m s \u001b[39m=\u001b[39m Series(values, index\u001b[39m=\u001b[39;49mkeys, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    567\u001b[0m \u001b[39m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mand\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/series.py:425\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    422\u001b[0m name \u001b[39m=\u001b[39m ibase\u001b[39m.\u001b[39mmaybe_extract_name(name, data, \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[1;32m    424\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dtype(dtype)\n",
      "File \u001b[0;32m/work/pi_mshao_umassd_edu/neeresh/envs/pytorch/lib/python3.8/site-packages/pandas/core/indexes/base.py:7103\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7072\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mensure_index\u001b[39m(index_like: Axes, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[1;32m   7073\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   7074\u001b[0m \u001b[39m    Ensure that we have an index from some index-like object.\u001b[39;00m\n\u001b[1;32m   7075\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7101\u001b[0m \u001b[39m           )\u001b[39;00m\n\u001b[1;32m   7102\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 7103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(index_like, Index):\n\u001b[1;32m   7104\u001b[0m         \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m   7105\u001b[0m             index_like \u001b[39m=\u001b[39m index_like\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "\n",
    "    # Gathering Files\n",
    "    # input_directory = os.path.join(project_root(), 'physionet.org', 'files', 'challenge-2019', '1.0.0', 'training','training_setA')\n",
    "    \n",
    "    input_directory = \"/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setA/\"\n",
    "    # input_directory = \"/localscratch/neeresh/data/physionet2019/physionet.org/files/challenge-2019/1.0.0/training/training_setB/\"\n",
    "    output_directory = \"./predictions\"\n",
    "\n",
    "    # Find files.\n",
    "    files = []\n",
    "    for f in os.listdir(input_directory):\n",
    "        if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('psv'):\n",
    "            files.append(f)\n",
    "    \n",
    "    # files.sort()\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    \n",
    "    # Load Sepsis Model\n",
    "    model = load_sepsis_model(d_input=d_input, d_channel=d_channel, d_output=d_output)\n",
    "\n",
    "    # Iterate over files.\n",
    "    print('Predicting sepsis labels...')\n",
    "    num_files = len(files)\n",
    "    for i, f in tqdm.tqdm(enumerate(files), desc=\"Remaining Files: \", total=num_files):\n",
    "        # print('    {}/{}...'.format(i+1, num_files))\n",
    "\n",
    "        # Load data.\n",
    "        input_file = os.path.join(input_directory, f)\n",
    "        data = load_challenge_data(input_file)\n",
    "\n",
    "        # Make predictions.\n",
    "        num_rows = len(data)  # Number of patient recordings\n",
    "        scores = np.zeros(num_rows)\n",
    "        labels = np.zeros(num_rows)\n",
    "        \n",
    "        for t in range(num_rows):\n",
    "            current_data = data[:t+1]\n",
    "            current_labels, current_score, data_df = get_sepsis_score(current_data, model)\n",
    "            scores[t] = current_score[0]\n",
    "            labels[t] = current_labels[0]\n",
    "        \n",
    "        output_file = os.path.join(output_directory, f)\n",
    "        save_challenge_predictions(output_file, scores, labels)\n",
    "    \n",
    "    return model, data, current_data, data_df\n",
    "\n",
    "model, data, current_data, data_df = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d948b978ff3e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:42.921579Z",
     "start_time": "2024-06-15T07:33:42.920147Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc2c458c15b85e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:22.330952Z",
     "start_time": "2024-06-15T07:33:22.329663Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.evaluate_sepsis_score import evaluate_sepsis_score\n",
    "\n",
    "# Numbers of label and prediction files must be the same\n",
    "evaluate_sepsis_score(label_directory='./labels/', prediction_directory='./predictions/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f1d95a6525fdae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:22.335205Z",
     "start_time": "2024-06-15T07:33:22.333655Z"
    }
   },
   "source": [
    "- **1. Load data from training_setA/training_setB only.**\n",
    "- **2. Divide it into train and test.**\n",
    "- **3. Train the model and evaluate it on test.**\n",
    "- **4. Use the evaluate method to get the predictions.**\n",
    "- **5. Run the get_true_labels for training_setA files.**\n",
    "- **6. run evaluate_sepsis_score.py use: from utils.evaluate_sepsis_score import evaluate_sepsis_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1870eeb818a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T07:33:22.337202Z",
     "start_time": "2024-06-15T07:33:22.335851Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034584d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea946b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from utils.evaluate_sepsis_score import evaluate_sepsis_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c96743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nperla_umassd_edu/Documents/SepsisEarlyPrediction/utils/evaluate_sepsis_score.py:114: UserWarning: Predictions are inconsistent with probabilities, i.e., a positive prediction has a lower (or equal) probability than a negative prediction.\n",
      "  warnings.warn('Predictions are inconsistent with probabilities, i.e., a positive prediction has a lower (or equal) probability than a negative prediction.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's ability to distinguish between positive and negative classes (AUROC): 0.7421306751717418\n",
      "Model's precision-recall trade-off (AUPRC): 0.0809671261481644\n",
      "Model's overall accuracy: 0.15723062710781244\n",
      "Model's balance between precision and recall (F-measure): 0.04804590176119706\n",
      "Normalized utility score: -0.1468316803384114\n"
     ]
    }
   ],
   "source": [
    "auroc, auprc, accuracy, f_measure, normalized_observed_utility = evaluate_sepsis_score(label_directory='./labels/', prediction_directory='./predictions_weight_ls/')\n",
    "\n",
    "print(f\"Model's ability to distinguish between positive and negative classes (AUROC): {auroc}\")\n",
    "print(f\"Model's precision-recall trade-off (AUPRC): {auprc}\")\n",
    "print(f\"Model's overall accuracy: {accuracy}\")\n",
    "print(f\"Model's balance between precision and recall (F-measure): {f_measure}\")\n",
    "print(f\"Normalized utility score: {normalized_observed_utility}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741deeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7421306751717418,\n",
       " 0.0809671261481644,\n",
       " 0.15723062710781244,\n",
       " 0.04804590176119706)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc, auprc, accuracy, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03a113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e24ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used: final_dataset.pickle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from utils.path_utils import project_root\n",
    "\n",
    "def initialize_experiment(data_file=None):\n",
    "\n",
    "    if data_file is not None:\n",
    "        data_file = \"training_ffill_bfill_zeros.pickle\"\n",
    "    data_file = \"final_dataset.pickle\"\n",
    "\n",
    "    print(f\"Dataset used: {data_file}\")\n",
    "\n",
    "    # [[patient1], [patient2], [patient3], ..., [patientN]]\n",
    "    training_examples = pd.read_pickle(os.path.join(project_root(), 'data', 'processed', data_file))\n",
    "\n",
    "    with open(os.path.join(project_root(), 'data', 'processed', 'lengths.txt')) as f:\n",
    "        lengths_list = [int(length) for length in f.read().splitlines()]\n",
    "    with open(os.path.join(project_root(), 'data', 'processed', 'is_sepsis.txt')) as f:\n",
    "        is_sepsis = [int(is_sep) for is_sep in f.read().splitlines()]\n",
    "\n",
    "    return training_examples, lengths_list, is_sepsis\n",
    "\n",
    "training_examples, lengths_list, is_sepsis = initialize_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2d74b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding...: 100%|██████████| 16268/16268 [00:27<00:00, 590.89it/s]\n",
      "Padding...: 100%|██████████| 4067/4067 [00:06<00:00, 587.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.loader import make_loader\n",
    "train_loader, test_loader, train_indicies, test_indicies = make_loader(training_examples, lengths_list, is_sepsis, 128, mode='padding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f1710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9f225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d672e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
